{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGSA KAGGLE CHALLENGE : MISSING LINK PREDICTION\n",
    "\n",
    "#### Team Name : Joel&Fatma&Joseph&Brice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des fichiers et séparation train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "y = [elt[-1] for elt in training_set]\n",
    "y = np.array(y).astype(int)\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idée rajouter titre à l'abstract\n",
    "corpus = [element[5] + ' ' + element[2].lower() for element in node_info]\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('stopwords'); nltk.download('punkt'); nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    result = [stemmer.stem(i) for i in tokens if((i.isalpha()) & (i not in stop_words)) ]\n",
    "    return ' '.join(result)\n",
    "## paralellisation de la lemmatisation des textes\n",
    "from multiprocessing import Pool\n",
    "with Pool(8) as p: \n",
    "    corpus_new = p.map(tokenize, corpus)\n",
    "\n",
    "## création d'un dataframe pour stocker les informations des noeuds\n",
    "dico_node_info = {elt[0]: {'pub_year': int(elt[1]), 'authors' : elt[3].split(','),\n",
    "                                'abstract' : corpus_new[i].split()} for i, elt in enumerate(node_info)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression volontaire de quelques liens pour valider le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/335130 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "data_ = pd.DataFrame({'node_1':[int(elt[0]) for elt in training_set], \n",
    "                     'node_2':[int(elt[1]) for elt in training_set],\n",
    "                    'link' : [int(elt[2]) for elt in training_set]})\n",
    "#fb_df_temp = data_[data_.link == 1]\n",
    "#fb_df_temp.link = 1 - fb_df_temp.link\n",
    "#fb_df_temp.columns = ['node_2', 'node_1', 'link']\n",
    "\n",
    "#data_ = data_.append(fb_df_temp)\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "fb_df_temp = data_[data_.link == 1]\n",
    "initial_node_count = len(set(fb_df_temp.node_1.unique()).union(set(fb_df_temp.node_2.unique())))\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "for i in tqdm.tqdm(data_[data_.link == 1].index.values):\n",
    "    break\n",
    "    # remove a node pair and build a new graph\n",
    "    tmp = fb_df_temp.drop(index = i) \n",
    "    all_nodes = set(tmp.node_1.unique()).union(set(tmp.node_2.unique()))\n",
    "    if len(all_nodes) == initial_node_count :\n",
    "        omissible_links_index.append(i)\n",
    "        fb_df_temp = fb_df_temp.drop(index = i)        \n",
    "    else :\n",
    "        pass\n",
    "    if len(omissible_links_index) == 15000 :\n",
    "        break\n",
    "#np.save('omissible.npy', omissible_links_index)\n",
    "# omissible_links_index = np.load('omissible.npy')[:15000]\n",
    "fb_df_partial = data_[data_.link == 1].drop(index = omissible_links_index).drop(columns=['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = set(fb_df_partial.node_1.unique()).union(set(fb_df_partial.node_2.unique()))\n",
    "nodes_bizz = [node for node in dico_node_info.keys() if int(node) not in all_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_data = nx.from_pandas_edgelist(fb_df_partial, \"node_1\", \"node_2\", create_using=nx.DiGraph())\n",
    "G_data.add_nodes_from(nodes_bizz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27770"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = [tuple(elt[:2]) for elt in np.array(training_set).astype(int)] +\\\n",
    "            [tuple(elt[:2]) for elt in np.array(testing_set).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  600512\n",
      "val : 15000\n"
     ]
    }
   ],
   "source": [
    "pairs_train = [tuple(elt[0 : 2]) for i, elt in enumerate(training_set) if i not in omissible_links_index]\n",
    "y_train = [int(elt[-1]) for i, elt in enumerate(training_set) if i not in omissible_links_index]\n",
    "print('train : ', len(pairs_train))\n",
    "\n",
    "pairs_val = [tuple(elt[0 : 2]) for i, elt in enumerate(training_set) if i in omissible_links_index]\n",
    "y_val = np.array([int(elt[-1]) for i, elt in enumerate(training_set) if i in omissible_links_index])\n",
    "print('val :', len(pairs_val))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with attributes of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27770, 8868)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TF-IDF pour la similarité entre abstract + titre\n",
    "vectorizer = TfidfVectorizer(decode_error='ignore', smooth_idf=False, stop_words=stop_words,\n",
    "                      encoding='utf-8', min_df = 2, max_df=5000)\n",
    "# each row is a node in the order of node_info\n",
    "corpus_idf = vectorizer.fit_transform(corpus_new)\n",
    "corpus_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6403697782720512 % des delta years < 0 sont connectés\n"
     ]
    }
   ],
   "source": [
    "## Etude du delta entre les années\n",
    "tmp_ = pd.DataFrame.from_dict(dico_node_info).T[['pub_year', 'authors']]\n",
    "tmp_['node_1'] = tmp_.index.astype(int)\n",
    "tmp = pd.merge(tmp_, data_, on='node_1')\n",
    "tmp = tmp.rename(columns = {\"pub_year\":\"pub_year_1\"})\n",
    "tmp_ = tmp_.rename(columns = {\"node_1\":\"node_2\"})\n",
    "tmp = pd.merge(tmp_, tmp, on='node_2')\n",
    "tmp = tmp.rename(columns = {\"pub_year\":\"pub_year_2\"})\n",
    "tmp['diff_year'] = tmp.pub_year_1 - tmp.pub_year_2\n",
    "x = 100*( len(tmp[(tmp.diff_year < 0 ) & (tmp.link == 1)])/(tmp.diff_year < 0 ).sum() )\n",
    "print(x, \"% des delta years < 0 sont connectés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc Embedding of the abstracts\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_new)]\n",
    "model = Doc2Vec(documents, vector_size=64, window=2, min_count=1, workers=7)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=100)\n",
    "fname ='doc2vec'\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27770/27770 [01:29<00:00, 309.56it/s]\n"
     ]
    }
   ],
   "source": [
    "abstr_embs = {} \n",
    "for source in tqdm.tqdm (dico_node_info.keys()) :\n",
    "    abstr_embs[source] = model.infer_vector(dico_node_info[source]['abstract'])\n",
    "def compute_similarity_emb(vector_s, vector_t):\n",
    "    sim = np.dot(vector_s, vector_t)/(np.linalg.norm(vector_s)*np.linalg.norm(vector_t))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tuple_to_process) : \n",
    "    ret = []\n",
    "    source = tuple_to_process[0]\n",
    "    target = tuple_to_process[1]\n",
    "    \n",
    "    source_info = dico_node_info[source]\n",
    "    target_info = dico_node_info[target]\n",
    "    \n",
    "    source_abstract = set(source_info['abstract'])\n",
    "    target_abstract = set(target_info['abstract'])\n",
    "    \n",
    "    source_auth = set(source_info['authors'])\n",
    "    target_auth = set(target_info['authors'])\n",
    "    \n",
    "    ## delta entre dates de publication\n",
    "    delta_year = int(source_info['pub_year']) - int(target_info['pub_year'])\n",
    "    \n",
    "    ## cosine similiarity et intersection auteurs abstracts\n",
    "    common_words = len(source_abstract.intersection(target_abstract))/(len(source_abstract.union(target_abstract)))\n",
    "    common_auths = len(source_auth.intersection(target_auth))/(len(source_auth.union(target_auth)))\n",
    "    \n",
    "    cosine_sim = corpus_idf[IDs.index(source)].dot( \n",
    "                        corpus_idf[IDs.index(target)].T).toarray()[0][0] ## les vecteurs sont déjà normés \n",
    "    \n",
    "    ret.append(common_words)\n",
    "    ret.append(cosine_sim) ## tf-idf similarity\n",
    "    ret.append(delta_year)\n",
    "    ret.append(common_auths)\n",
    "    ret.append(compute_similarity_emb(abstr_embs[source], abstr_embs[target]))\n",
    "    \n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features liées à la structure du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(G, ebunch = None):\n",
    "    u = ebunch[0]\n",
    "    v = ebunch[1]\n",
    "    if str(u) in nodes_bizz :\n",
    "        u = str(u)\n",
    "    if str(v) in nodes_bizz :\n",
    "        v = str(v)\n",
    "    common_neighbors =  [w for w in G[u] if w in G[v] and w not in (u, v)]\n",
    "    return common_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648160/648160 [00:22<00:00, 29168.33it/s]\n"
     ]
    }
   ],
   "source": [
    "dico_common_neighbors = {}\n",
    "for elt in tqdm.tqdm(all_pairs) :\n",
    "    dico_common_neighbors[elt] = common_neighbors(G_data, elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph similarity measures\n",
    "def resource_allocation_index(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        return sum(1 / G.degree(w) for w in dico_common_neighbors.get(key_, []))\n",
    "    return predict(ebunch[0], ebunch[1])\n",
    "def jaccard_coefficient(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        cnbors = list(dico_common_neighbors.get(key_,[]))\n",
    "        union_size = len(set(G[u]) | set(G[v]))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return len(cnbors) / union_size    \n",
    "    return predict(ebunch[0], ebunch[1])\n",
    "import math\n",
    "def adamic_adar_index(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "  \n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        try : \n",
    "            return sum(1 / math.log(G.degree(w)) for w in dico_common_neighbors.get(key_, []))\n",
    "        except :\n",
    "            print(\"bizzare\")\n",
    "            return -1\n",
    "  \n",
    "    return predict(ebunch[0], ebunch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shortest path\n",
    "def shortest_path(source, target, G_data) :\n",
    "    try :\n",
    "        G_data.remove_edge(source, target)\n",
    "        try : \n",
    "            ret = nx.shortest_path_length(G_data, source, target)\n",
    "            G_data.add_edge(source, target)\n",
    "            return ret\n",
    "        except :\n",
    "            G_data.add_edge(source, target)\n",
    "            return 1000\n",
    "    except :\n",
    "        try : \n",
    "            ret = nx.shortest_path_length(G_data, source, target)\n",
    "            return ret\n",
    "        except :\n",
    "            return 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Centrality, page rank and HITS\n",
    "out_degree_centrality =  nx.out_degree_centrality(G_data)\n",
    "in_degree_centrality = nx.in_degree_centrality(G_data)\n",
    "page_rank = nx.pagerank_scipy(G_data)\n",
    "hub_score, authority_score = nx.hits(G_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_graph_feature(tuple_to_process, G_data=G_data) :\n",
    "    ret = []\n",
    "    source = tuple_to_process[0]\n",
    "    target = tuple_to_process[1]\n",
    "    if source not in nodes_bizz :\n",
    "        source = int(source)\n",
    "    if target not in nodes_bizz :\n",
    "        target = int(target)\n",
    "    ebunch = (source, target)    \n",
    "    ret.append(out_degree_centrality[source])\n",
    "    ret.append(in_degree_centrality[target])\n",
    "    ret.append(page_rank[target])\n",
    "    ret.append(out_degree_centrality[source] * in_degree_centrality[target])\n",
    "    ret.append(hub_score[source])\n",
    "    ret.append(authority_score[target])\n",
    "    try :\n",
    "        cm = len(dico_common_neighbors.get(ebunch, []))/(len(G_data[source]) + len(G_data[target]))\n",
    "    except :\n",
    "        cm = 0\n",
    "    ret.append(cm)\n",
    "    ret.append(resource_allocation_index(G_data, ebunch))\n",
    "    ret.append(jaccard_coefficient(G_data, ebunch))\n",
    "    ret.append(adamic_adar_index(G_data, ebunch))\n",
    "    ret.append(shortest_path(source , target, G_data))\n",
    "\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(6) as p: \n",
    "    features_nodes = p.map(get_features, pairs_train)\n",
    "features_nodes = np.array(features_nodes)\n",
    "\n",
    "with Pool(6) as p: \n",
    "    train_tr = p.map(get_graph_feature, pairs_train )\n",
    "train_tr = np.asarray(train_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600512, 5)\n",
      "(600512, 11)\n"
     ]
    }
   ],
   "source": [
    "print(features_nodes.shape)\n",
    "print(train_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(6) as p: \n",
    "    features_val = p.map(get_features, pairs_val)\n",
    "features_val = np.array(features_val)\n",
    "\n",
    "with Pool(6) as p: \n",
    "    val_tr = p.map(get_graph_feature, pairs_val)\n",
    "val_tr = np.asarray(val_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 5)\n",
      "(15000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(features_val.shape)\n",
    "print(val_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "def train_logistic(train, val, c_inf=-1, c_up=2, n=20) : \n",
    "    logistic = linear_model.LogisticRegression()\n",
    "    C = np.logspace(c_inf, c_up, n)\n",
    "    params = dict(C=C, class_weight=['balanced'], solver=['lbfgs'])\n",
    "    clf = GridSearchCV(logistic, params, cv=3, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "    clf.fit(train, y_train)\n",
    "    y_pred = clf.predict(val)\n",
    "    score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "    print(score)\n",
    "    return clf, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize basic SVM\n",
    "def train_svm(train, val, c_inf=-1, c_up=2, n=5) : \n",
    "    classifier = svm.LinearSVC()\n",
    "    C = np.logspace(c_inf, c_up, n)\n",
    "    params = dict(C=C, class_weight=['balanced'])\n",
    "    clf = GridSearchCV(classifier, params, cv=2, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "    clf.fit(train, y_train)\n",
    "    y_pred = clf.predict(val)\n",
    "    score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "    print(score)\n",
    "    return clf, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## f-1 score\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_score = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec uniquement les features graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600512, 11) (15000, 11)\n"
     ]
    }
   ],
   "source": [
    "train = scaler.fit_transform(train_tr)\n",
    "val = scaler.transform(val_tr)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:   30.6s remaining:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   32.3s remaining:    9.8s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:   38.1s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   38.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9215615788338486\n"
     ]
    }
   ],
   "source": [
    "## Logistic regression\n",
    "clf, score = train_logistic(train, val, c_inf=2, c_up=3, n=10)\n",
    "clf.best_params_\n",
    "dico_score['graph_structure_logistic'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:  1.9min remaining:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  2.0min remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  2.2min remaining:   56.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.914655764986795\n",
      "{'C': 3.1622776601683795, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "clf, score = train_svm(train, val)\n",
    "print(clf.best_params_)\n",
    "dico_score['graph_structure_svm'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618658730708007\n"
     ]
    }
   ],
   "source": [
    "## XGBOOST\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {'subsample': 0.7,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0.1,\n",
    " 'n_estimators': 200,\n",
    " 'min_samples_split': 3,\n",
    " 'min_samples_leaf': 3,\n",
    " 'min_child_weight': 7,\n",
    " 'max_depth': 30,\n",
    " 'learning_rate': 0.1,\n",
    " 'gamma': 0.1,\n",
    " 'criterion': 'entropy',\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 0.7,\n",
    "         'n_jobs':-1}\n",
    "clf = XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "clf.fit(train, y_train)\n",
    "y_pred = clf.predict(val)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['graph_structure_xgboost'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>shortest_path</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>resource</td>\n",
       "      <td>49.721283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>common_neighs</td>\n",
       "      <td>36.903740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>29.411633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>adamic</td>\n",
       "      <td>5.706720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>page_rank</td>\n",
       "      <td>3.162671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>authority</td>\n",
       "      <td>1.972737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pref_att</td>\n",
       "      <td>1.938154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>hub_score</td>\n",
       "      <td>1.897993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in_deg</td>\n",
       "      <td>1.599373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>out_deg</td>\n",
       "      <td>1.513994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Variable  Importance\n",
       "10  shortest_path  100.000000\n",
       "7        resource   49.721283\n",
       "6   common_neighs   36.903740\n",
       "8         jaccard   29.411633\n",
       "9          adamic    5.706720\n",
       "2       page_rank    3.162671\n",
       "5       authority    1.972737\n",
       "3        pref_att    1.938154\n",
       "4       hub_score    1.897993\n",
       "1          in_deg    1.599373\n",
       "0         out_deg    1.513994"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['out_deg', 'in_deg', 'page_rank','pref_att',\n",
    "           'hub_score','authority', 'common_neighs', 'resource',\n",
    "           'jaccard', 'adamic', 'shortest_path']\n",
    "feature_importance = pd.DataFrame(clf.feature_importances_)\n",
    "feature_importance = 100.0 * \\\n",
    "    (feature_importance / feature_importance.max())\n",
    "feature_importance = pd.concat([pd.DataFrame(\n",
    "    columns), feature_importance], axis=1, ignore_index=True)\n",
    "feature_importance.columns = ['Variable', 'Importance']\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by='Importance', axis=0, ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec uniquement les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600512, 5) (15000, 5)\n"
     ]
    }
   ],
   "source": [
    "train = scaler.fit_transform(features_nodes) #features_nodes #\n",
    "val = scaler.transform(features_val) #features_val #\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:   17.6s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   18.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8777495136914559\n",
      "{'C': 0.30391953823131973, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "## Logistic regression\n",
    "clf, score = train_logistic(train, val, c_inf=-1, c_up=1, n=30)\n",
    "print(clf.best_params_)\n",
    "dico_score['node_attribut_logistic'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   52.5s remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.6min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  1.8min remaining:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716289916124421\n",
      "{'C': 17.78279410038923, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "clf, score = train_svm(train, val)\n",
    "print(clf.best_params_)\n",
    "dico_score['node_attribut_svm'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9214064859423311\n"
     ]
    }
   ],
   "source": [
    "## XGBOOST\n",
    "clf = XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "clf.fit(train, y_train)\n",
    "y_pred = clf.predict(val)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['node_attribut_xgboost'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tf_idf_cosine</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>common_authors</td>\n",
       "      <td>46.640224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>diff_year</td>\n",
       "      <td>34.547367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>common_words</td>\n",
       "      <td>14.835013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>doc2vec_cosine</td>\n",
       "      <td>9.219380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Variable  Importance\n",
       "2   tf_idf_cosine  100.000000\n",
       "1  common_authors   46.640224\n",
       "3       diff_year   34.547367\n",
       "0    common_words   14.835013\n",
       "4  doc2vec_cosine    9.219380"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_attr = ['common_words', 'common_authors', 'tf_idf_cosine', 'diff_year', 'doc2vec_cosine']\n",
    "feature_importance = pd.DataFrame(clf.feature_importances_)\n",
    "feature_importance = 100.0 * \\\n",
    "    (feature_importance / feature_importance.max())\n",
    "feature_importance = pd.concat([pd.DataFrame(\n",
    "    columns_attr), feature_importance], axis=1, ignore_index=True)\n",
    "feature_importance.columns = ['Variable', 'Importance']\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by='Importance', axis=0, ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En concatenant les deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600512, 16) (15000, 16)\n"
     ]
    }
   ],
   "source": [
    "train = np.hstack((train_tr, features_nodes))\n",
    "val = np.hstack((val_tr, features_val))\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:   35.6s remaining:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:   37.1s remaining:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:   44.4s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   44.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9573196162936187\n",
      "{'C': 10.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "clf, score = train_logistic(train, val, c_inf=1, c_up=3, n=10)\n",
    "print(clf.best_params_)\n",
    "dico_score['graph_attribut_logistic'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:  1.5min remaining:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  10 | elapsed:  1.8min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  2.1min remaining:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9556135770234987\n",
      "{'C': 17.78279410038923, 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "clf, score = train_svm(train, val)\n",
    "print(clf.best_params_)\n",
    "dico_score['node_attribut_svm'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9722507708119219\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>shortest_path</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>common_neighs</td>\n",
       "      <td>42.303875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>resource</td>\n",
       "      <td>39.663643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>adamic</td>\n",
       "      <td>12.495809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>jaccard</td>\n",
       "      <td>8.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>tf_idf_cosine</td>\n",
       "      <td>6.119644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>common_authors</td>\n",
       "      <td>3.851392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>diff_year</td>\n",
       "      <td>3.079254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>page_rank</td>\n",
       "      <td>1.465281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pref_att</td>\n",
       "      <td>1.279755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>authority</td>\n",
       "      <td>1.108819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>hub_score</td>\n",
       "      <td>1.108503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in_deg</td>\n",
       "      <td>1.038362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>common_words</td>\n",
       "      <td>1.020769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>out_deg</td>\n",
       "      <td>0.863967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>doc2vec_cosine</td>\n",
       "      <td>0.685441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Variable  Importance\n",
       "10   shortest_path  100.000000\n",
       "6    common_neighs   42.303875\n",
       "7         resource   39.663643\n",
       "9           adamic   12.495809\n",
       "8          jaccard    8.823529\n",
       "13   tf_idf_cosine    6.119644\n",
       "12  common_authors    3.851392\n",
       "14       diff_year    3.079254\n",
       "2        page_rank    1.465281\n",
       "3         pref_att    1.279755\n",
       "5        authority    1.108819\n",
       "4        hub_score    1.108503\n",
       "1           in_deg    1.038362\n",
       "11    common_words    1.020769\n",
       "0          out_deg    0.863967\n",
       "15  doc2vec_cosine    0.685441"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "clf.fit(train, y_train)\n",
    "y_pred = clf.predict(val)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['graph_attribut_xgboost'] = score\n",
    "feature_importance = pd.DataFrame(clf.feature_importances_)\n",
    "feature_importance = 100.0 * \\\n",
    "    (feature_importance / feature_importance.max())\n",
    "feature_importance = pd.concat([pd.DataFrame(\n",
    "    columns + columns_attr), feature_importance], axis=1, ignore_index=True)\n",
    "feature_importance.columns = ['Variable', 'Importance']\n",
    "feature_importance = feature_importance.sort_values(\n",
    "    by='Importance', axis=0, ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jores/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jores/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 600512 samples, validate on 15000 samples\n",
      "Epoch 1/30\n",
      "600512/600512 [==============================] - 2s 3us/sample - loss: 0.2803 - f1_m: 0.9058 - val_loss: 0.1632 - val_f1_m: 0.9640\n",
      "Epoch 2/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1656 - f1_m: 0.9435 - val_loss: 0.1433 - val_f1_m: 0.9705\n",
      "Epoch 3/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1508 - f1_m: 0.9483 - val_loss: 0.1332 - val_f1_m: 0.9730\n",
      "Epoch 4/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1451 - f1_m: 0.9501 - val_loss: 0.1241 - val_f1_m: 0.9755\n",
      "Epoch 5/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1423 - f1_m: 0.9510 - val_loss: 0.1283 - val_f1_m: 0.9743\n",
      "Epoch 6/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1398 - f1_m: 0.9518 - val_loss: 0.1300 - val_f1_m: 0.9741\n",
      "Epoch 7/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1377 - f1_m: 0.9524 - val_loss: 0.1281 - val_f1_m: 0.9735\n",
      "Epoch 8/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1361 - f1_m: 0.9530 - val_loss: 0.1312 - val_f1_m: 0.9728\n",
      "Epoch 9/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1352 - f1_m: 0.9535 - val_loss: 0.1320 - val_f1_m: 0.9729\n",
      "Epoch 10/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1338 - f1_m: 0.9538 - val_loss: 0.1216 - val_f1_m: 0.9745\n",
      "Epoch 11/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1330 - f1_m: 0.9541 - val_loss: 0.1256 - val_f1_m: 0.9736\n",
      "Epoch 12/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1320 - f1_m: 0.9542 - val_loss: 0.1299 - val_f1_m: 0.9727\n",
      "Epoch 13/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1315 - f1_m: 0.9546 - val_loss: 0.1161 - val_f1_m: 0.9762\n",
      "Epoch 14/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1312 - f1_m: 0.9549 - val_loss: 0.1170 - val_f1_m: 0.9757\n",
      "Epoch 15/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1304 - f1_m: 0.9550 - val_loss: 0.1250 - val_f1_m: 0.9737\n",
      "Epoch 16/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1297 - f1_m: 0.9551 - val_loss: 0.1248 - val_f1_m: 0.9748\n",
      "Epoch 17/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1290 - f1_m: 0.9555 - val_loss: 0.1084 - val_f1_m: 0.9774\n",
      "Epoch 18/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1285 - f1_m: 0.9554 - val_loss: 0.1368 - val_f1_m: 0.9723\n",
      "Epoch 19/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1278 - f1_m: 0.9558 - val_loss: 0.1180 - val_f1_m: 0.9753\n",
      "Epoch 20/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1276 - f1_m: 0.9559 - val_loss: 0.1099 - val_f1_m: 0.9770\n",
      "Epoch 21/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1273 - f1_m: 0.9559 - val_loss: 0.1048 - val_f1_m: 0.9777\n",
      "Epoch 22/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1268 - f1_m: 0.9562 - val_loss: 0.1219 - val_f1_m: 0.9750\n",
      "Epoch 23/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1261 - f1_m: 0.9563 - val_loss: 0.1065 - val_f1_m: 0.9773\n",
      "Epoch 24/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1257 - f1_m: 0.9565 - val_loss: 0.1127 - val_f1_m: 0.9769\n",
      "Epoch 25/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1255 - f1_m: 0.9566 - val_loss: 0.1274 - val_f1_m: 0.9739\n",
      "Epoch 26/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1249 - f1_m: 0.9567 - val_loss: 0.1202 - val_f1_m: 0.9748\n",
      "Epoch 27/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1245 - f1_m: 0.9569 - val_loss: 0.1311 - val_f1_m: 0.9736\n",
      "Epoch 28/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1246 - f1_m: 0.9569 - val_loss: 0.1346 - val_f1_m: 0.9739\n",
      "Epoch 29/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1240 - f1_m: 0.9572 - val_loss: 0.1311 - val_f1_m: 0.9729\n",
      "Epoch 30/30\n",
      "600512/600512 [==============================] - 1s 2us/sample - loss: 0.1235 - f1_m: 0.9573 - val_loss: 0.1254 - val_f1_m: 0.9744\n",
      "0.9742186965738905\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input, BatchNormalization\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 4096\n",
    "n_epochs = 30\n",
    "history = model.fit(train, np.array(y_train),\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(val, np.array(y_val)))\n",
    "y_pred = model.predict(val)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['graph_attribut_neural_network'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node embeddings + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement du Node2Vec\n",
    "G_data = nx.from_pandas_edgelist(fb_df_partial, \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "G_data.add_nodes_from(nodes_bizz)\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G_data, dimensions=128, walk_length=30, \n",
    "                    num_walks=30, workers=1, temp_folder='')\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=7, min_count=1)\n",
    "n2w_model.wv.save_word2vec_format('node_emb_partiel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chargement du modèle\n",
    "from gensim.models import KeyedVectors\n",
    "n2w_model = KeyedVectors.load_word2vec_format('node_emb_partiel', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_emb(data) :\n",
    "    node_emb = []\n",
    "    for i, j in zip(data['node_1'], data['node_2']) :\n",
    "        if i in G_data.nodes():\n",
    "            emb_i = n2w_model.wv[str(i)]\n",
    "        else :\n",
    "            emb_i = np.zeros(128)\n",
    "        if j in G_data.nodes():\n",
    "            emb_j = n2w_model.wv[str(j)]\n",
    "        else :\n",
    "            emb_j = np.zeros(128)\n",
    "        node_emb.append(np.hstack((emb_i, emb_j)))\n",
    "    node_emb = np.asarray(node_emb)\n",
    "    return node_emb )\n",
    "node_emb = get_node_emb(data_.drop(index = omissible_links_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600512, 256)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Validation sur les noeuds omis !!\n",
    "omitted_links = data_[data_.link == 1].iloc[omissible_links_index]\n",
    "omitted_emb = get_node_emb(omitted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training only on node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600512 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.3644 - f1_m: 0.8293 - val_loss: 0.0640 - val_f1_m: 0.9897\n",
      "Epoch 2/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0845 - f1_m: 0.9766 - val_loss: 0.0308 - val_f1_m: 0.9953\n",
      "Epoch 3/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0501 - f1_m: 0.9868 - val_loss: 0.0205 - val_f1_m: 0.9973\n",
      "Epoch 4/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0388 - f1_m: 0.9900 - val_loss: 0.0194 - val_f1_m: 0.9969\n",
      "Epoch 5/50\n",
      "600512/600512 [==============================] - 14s 24us/sample - loss: 0.0331 - f1_m: 0.9916 - val_loss: 0.0137 - val_f1_m: 0.9982\n",
      "Epoch 6/50\n",
      "600512/600512 [==============================] - 15s 25us/sample - loss: 0.0284 - f1_m: 0.9929 - val_loss: 0.0127 - val_f1_m: 0.9981\n",
      "Epoch 7/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0243 - f1_m: 0.9940 - val_loss: 0.0063 - val_f1_m: 0.9992\n",
      "Epoch 8/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0242 - f1_m: 0.9938 - val_loss: 0.0069 - val_f1_m: 0.9990\n",
      "Epoch 9/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0218 - f1_m: 0.9946 - val_loss: 0.0095 - val_f1_m: 0.9986\n",
      "Epoch 10/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0215 - f1_m: 0.9946 - val_loss: 0.0096 - val_f1_m: 0.9984\n",
      "Epoch 11/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0202 - f1_m: 0.9950 - val_loss: 0.0092 - val_f1_m: 0.9984\n",
      "Epoch 12/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0182 - f1_m: 0.9955 - val_loss: 0.0072 - val_f1_m: 0.9989\n",
      "Epoch 13/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0165 - f1_m: 0.9959 - val_loss: 0.0066 - val_f1_m: 0.9989\n",
      "Epoch 14/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0177 - f1_m: 0.9956 - val_loss: 0.0063 - val_f1_m: 0.9991\n",
      "Epoch 15/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0143 - f1_m: 0.9966 - val_loss: 0.0060 - val_f1_m: 0.9990\n",
      "Epoch 16/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0136 - f1_m: 0.9968 - val_loss: 0.0057 - val_f1_m: 0.9993\n",
      "Epoch 17/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0127 - f1_m: 0.9970 - val_loss: 0.0070 - val_f1_m: 0.9992\n",
      "Epoch 18/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0117 - f1_m: 0.9972 - val_loss: 0.0061 - val_f1_m: 0.9990\n",
      "Epoch 19/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0132 - f1_m: 0.9968 - val_loss: 0.0084 - val_f1_m: 0.9987\n",
      "Epoch 20/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0130 - f1_m: 0.9969 - val_loss: 0.0052 - val_f1_m: 0.9990\n",
      "Epoch 21/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0098 - f1_m: 0.9975 - val_loss: 0.0088 - val_f1_m: 0.9987\n",
      "Epoch 22/50\n",
      "600512/600512 [==============================] - 14s 24us/sample - loss: 0.0092 - f1_m: 0.9975 - val_loss: 0.0068 - val_f1_m: 0.9991\n",
      "Epoch 23/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0084 - f1_m: 0.9974 - val_loss: 0.0070 - val_f1_m: 0.9988\n",
      "Epoch 24/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0102 - f1_m: 0.9971 - val_loss: 0.0053 - val_f1_m: 0.9991\n",
      "Epoch 25/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0072 - f1_m: 0.9978 - val_loss: 0.0078 - val_f1_m: 0.9986\n",
      "Epoch 26/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0090 - f1_m: 0.9972 - val_loss: 0.0083 - val_f1_m: 0.9990\n",
      "Epoch 27/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0089 - f1_m: 0.9974 - val_loss: 0.0077 - val_f1_m: 0.9991\n",
      "Epoch 28/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0077 - f1_m: 0.9978 - val_loss: 0.0061 - val_f1_m: 0.9990\n",
      "Epoch 29/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0062 - f1_m: 0.9981 - val_loss: 0.0042 - val_f1_m: 0.9992\n",
      "Epoch 30/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0064 - f1_m: 0.9981 - val_loss: 0.0075 - val_f1_m: 0.9988\n",
      "Epoch 31/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0057 - f1_m: 0.9982 - val_loss: 0.0315 - val_f1_m: 0.9962\n",
      "Epoch 32/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0088 - f1_m: 0.9975 - val_loss: 0.0061 - val_f1_m: 0.9989\n",
      "Epoch 33/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0059 - f1_m: 0.9983 - val_loss: 0.0053 - val_f1_m: 0.9994\n",
      "Epoch 34/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0065 - f1_m: 0.9981 - val_loss: 0.0052 - val_f1_m: 0.9993\n",
      "Epoch 35/50\n",
      "600512/600512 [==============================] - 14s 23us/sample - loss: 0.0052 - f1_m: 0.9985 - val_loss: 0.0065 - val_f1_m: 0.9989\n",
      "Epoch 36/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0053 - f1_m: 0.9984 - val_loss: 0.0070 - val_f1_m: 0.9990\n",
      "Epoch 37/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0051 - f1_m: 0.9984 - val_loss: 0.0050 - val_f1_m: 0.9993\n",
      "Epoch 38/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0053 - f1_m: 0.9984 - val_loss: 0.0040 - val_f1_m: 0.9993\n",
      "Epoch 39/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0047 - f1_m: 0.9986 - val_loss: 0.0045 - val_f1_m: 0.9993\n",
      "Epoch 40/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0062 - f1_m: 0.9982 - val_loss: 0.0065 - val_f1_m: 0.9991\n",
      "Epoch 41/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0045 - f1_m: 0.9986 - val_loss: 0.0046 - val_f1_m: 0.9993\n",
      "Epoch 42/50\n",
      "600512/600512 [==============================] - 12s 21us/sample - loss: 0.0050 - f1_m: 0.9984 - val_loss: 0.0032 - val_f1_m: 0.9994\n",
      "Epoch 43/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0046 - f1_m: 0.9986 - val_loss: 0.0062 - val_f1_m: 0.9990\n",
      "Epoch 44/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0048 - f1_m: 0.9986 - val_loss: 0.0054 - val_f1_m: 0.9992\n",
      "Epoch 45/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0048 - f1_m: 0.9986 - val_loss: 0.0073 - val_f1_m: 0.9990\n",
      "Epoch 46/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0051 - f1_m: 0.9984 - val_loss: 0.0050 - val_f1_m: 0.9992\n",
      "Epoch 47/50\n",
      "600512/600512 [==============================] - 14s 24us/sample - loss: 0.0073 - f1_m: 0.9980 - val_loss: 0.0052 - val_f1_m: 0.9991\n",
      "Epoch 48/50\n",
      "600512/600512 [==============================] - 13s 21us/sample - loss: 0.0043 - f1_m: 0.9987 - val_loss: 0.0048 - val_f1_m: 0.9992\n",
      "Epoch 49/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0039 - f1_m: 0.9989 - val_loss: 0.0051 - val_f1_m: 0.9993\n",
      "Epoch 50/50\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.0041 - f1_m: 0.9988 - val_loss: 0.0051 - val_f1_m: 0.9992\n",
      "0.9991659716430359\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 10000\n",
    "n_epochs = 50\n",
    "history = model.fit(node_emb, y_train,\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(omitted_emb, y_val))\n",
    "y_pred = model.predict(omitted_emb)\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['node2vec_alone_neural_network'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on node embedding + graph structure + nodes attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack([node_emb, train])\n",
    "x_val = np.hstack([omitted_emb, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic neural network with the concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600512 samples, validate on 15000 samples\n",
      "Epoch 1/30\n",
      "600512/600512 [==============================] - 13s 22us/sample - loss: 0.1809 - f1_m: 0.9311 - val_loss: 0.0264 - val_f1_m: 0.9960\n",
      "Epoch 2/30\n",
      "600512/600512 [==============================] - 10s 17us/sample - loss: 0.0517 - f1_m: 0.9846 - val_loss: 0.0171 - val_f1_m: 0.9977\n",
      "Epoch 3/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0365 - f1_m: 0.9894 - val_loss: 0.0175 - val_f1_m: 0.9979\n",
      "Epoch 4/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0284 - f1_m: 0.9917 - val_loss: 0.0095 - val_f1_m: 0.9984\n",
      "Epoch 5/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0220 - f1_m: 0.9937 - val_loss: 0.0122 - val_f1_m: 0.9980\n",
      "Epoch 6/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0194 - f1_m: 0.9944 - val_loss: 0.0119 - val_f1_m: 0.9981\n",
      "Epoch 7/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0159 - f1_m: 0.9953 - val_loss: 0.0172 - val_f1_m: 0.9966\n",
      "Epoch 8/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0155 - f1_m: 0.9954 - val_loss: 0.0107 - val_f1_m: 0.9986\n",
      "Epoch 9/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0139 - f1_m: 0.9959 - val_loss: 0.0082 - val_f1_m: 0.9989\n",
      "Epoch 10/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0128 - f1_m: 0.9962 - val_loss: 0.0092 - val_f1_m: 0.9988\n",
      "Epoch 11/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0111 - f1_m: 0.9966 - val_loss: 0.0064 - val_f1_m: 0.9991\n",
      "Epoch 12/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0101 - f1_m: 0.9969 - val_loss: 0.0085 - val_f1_m: 0.9987\n",
      "Epoch 13/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0086 - f1_m: 0.9974 - val_loss: 0.0081 - val_f1_m: 0.9987\n",
      "Epoch 14/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0085 - f1_m: 0.9974 - val_loss: 0.0154 - val_f1_m: 0.9976\n",
      "Epoch 15/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0077 - f1_m: 0.9977 - val_loss: 0.0080 - val_f1_m: 0.9990\n",
      "Epoch 16/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0072 - f1_m: 0.9977 - val_loss: 0.0068 - val_f1_m: 0.9989\n",
      "Epoch 17/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0071 - f1_m: 0.9978 - val_loss: 0.0074 - val_f1_m: 0.9988\n",
      "Epoch 18/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0065 - f1_m: 0.9980 - val_loss: 0.0143 - val_f1_m: 0.9975\n",
      "Epoch 19/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0064 - f1_m: 0.9980 - val_loss: 0.0158 - val_f1_m: 0.9976\n",
      "Epoch 20/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0056 - f1_m: 0.9983 - val_loss: 0.0043 - val_f1_m: 0.9991\n",
      "Epoch 21/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0050 - f1_m: 0.9984 - val_loss: 0.0079 - val_f1_m: 0.9986\n",
      "Epoch 22/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0045 - f1_m: 0.9986 - val_loss: 0.0098 - val_f1_m: 0.9989\n",
      "Epoch 23/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0047 - f1_m: 0.9985 - val_loss: 0.0106 - val_f1_m: 0.9987\n",
      "Epoch 24/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0048 - f1_m: 0.9984 - val_loss: 0.0067 - val_f1_m: 0.9988\n",
      "Epoch 25/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0049 - f1_m: 0.9983 - val_loss: 0.0183 - val_f1_m: 0.9970\n",
      "Epoch 26/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0069 - f1_m: 0.9978 - val_loss: 0.0100 - val_f1_m: 0.9982\n",
      "Epoch 27/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0047 - f1_m: 0.9985 - val_loss: 0.0122 - val_f1_m: 0.9980\n",
      "Epoch 28/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0044 - f1_m: 0.9986 - val_loss: 0.0176 - val_f1_m: 0.9972\n",
      "Epoch 29/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0043 - f1_m: 0.9987 - val_loss: 0.0072 - val_f1_m: 0.9988\n",
      "Epoch 30/30\n",
      "600512/600512 [==============================] - 11s 18us/sample - loss: 0.0036 - f1_m: 0.9989 - val_loss: 0.0115 - val_f1_m: 0.9982\n",
      "0.9981967541574834\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 10000\n",
    "n_epochs = 30\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(x_val, y_val))\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['all_features_neural_network_1'] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600512 samples, validate on 15000 samples\n",
      "Epoch 1/40\n",
      "600512/600512 [==============================] - 18s 29us/sample - loss: 0.2691 - f1_m: 0.8870 - val_loss: 0.0448 - val_f1_m: 0.9924\n",
      "Epoch 2/40\n",
      "600512/600512 [==============================] - 17s 28us/sample - loss: 0.0558 - f1_m: 0.9841 - val_loss: 0.0248 - val_f1_m: 0.9966\n",
      "Epoch 3/40\n",
      "600512/600512 [==============================] - 18s 29us/sample - loss: 0.0376 - f1_m: 0.9895 - val_loss: 0.0157 - val_f1_m: 0.9981\n",
      "Epoch 4/40\n",
      "600512/600512 [==============================] - 16s 27us/sample - loss: 0.0294 - f1_m: 0.9917 - val_loss: 0.0086 - val_f1_m: 0.9987\n",
      "Epoch 5/40\n",
      "600512/600512 [==============================] - 16s 27us/sample - loss: 0.0248 - f1_m: 0.9929 - val_loss: 0.0068 - val_f1_m: 0.9994\n",
      "Epoch 6/40\n",
      "600512/600512 [==============================] - 17s 28us/sample - loss: 0.0215 - f1_m: 0.9939 - val_loss: 0.0079 - val_f1_m: 0.9991\n",
      "Epoch 7/40\n",
      "600512/600512 [==============================] - 17s 28us/sample - loss: 0.0179 - f1_m: 0.9950 - val_loss: 0.0065 - val_f1_m: 0.9993\n",
      "Epoch 8/40\n",
      "600512/600512 [==============================] - 16s 27us/sample - loss: 0.0148 - f1_m: 0.9958 - val_loss: 0.0057 - val_f1_m: 0.9991\n",
      "Epoch 9/40\n",
      "600512/600512 [==============================] - 16s 27us/sample - loss: 0.0137 - f1_m: 0.9960 - val_loss: 0.0097 - val_f1_m: 0.9986\n",
      "Epoch 10/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0133 - f1_m: 0.9960 - val_loss: 0.0069 - val_f1_m: 0.9991\n",
      "Epoch 11/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0138 - f1_m: 0.9959 - val_loss: 0.0044 - val_f1_m: 0.9995\n",
      "Epoch 12/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0120 - f1_m: 0.9965 - val_loss: 0.0059 - val_f1_m: 0.9992\n",
      "Epoch 13/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0096 - f1_m: 0.9972 - val_loss: 0.0051 - val_f1_m: 0.9993\n",
      "Epoch 14/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0091 - f1_m: 0.9972 - val_loss: 0.0066 - val_f1_m: 0.9989\n",
      "Epoch 15/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0091 - f1_m: 0.9972 - val_loss: 0.0037 - val_f1_m: 0.9995\n",
      "Epoch 16/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0098 - f1_m: 0.9970 - val_loss: 0.0059 - val_f1_m: 0.9992\n",
      "Epoch 17/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0074 - f1_m: 0.9978 - val_loss: 0.0051 - val_f1_m: 0.9993\n",
      "Epoch 18/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0085 - f1_m: 0.9974 - val_loss: 0.0064 - val_f1_m: 0.9990\n",
      "Epoch 19/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0065 - f1_m: 0.9981 - val_loss: 0.0052 - val_f1_m: 0.9991\n",
      "Epoch 20/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0064 - f1_m: 0.9979 - val_loss: 0.0044 - val_f1_m: 0.9994\n",
      "Epoch 21/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0060 - f1_m: 0.9982 - val_loss: 0.0041 - val_f1_m: 0.9994\n",
      "Epoch 22/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0053 - f1_m: 0.9983 - val_loss: 0.0080 - val_f1_m: 0.9988\n",
      "Epoch 23/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0071 - f1_m: 0.9978 - val_loss: 0.0045 - val_f1_m: 0.9994\n",
      "Epoch 24/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0060 - f1_m: 0.9982 - val_loss: 0.0032 - val_f1_m: 0.9996\n",
      "Epoch 25/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0051 - f1_m: 0.9984 - val_loss: 0.0049 - val_f1_m: 0.9993\n",
      "Epoch 26/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0053 - f1_m: 0.9982 - val_loss: 0.0055 - val_f1_m: 0.9993\n",
      "Epoch 27/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0067 - f1_m: 0.9979 - val_loss: 0.0071 - val_f1_m: 0.9991\n",
      "Epoch 28/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0060 - f1_m: 0.9981 - val_loss: 0.0031 - val_f1_m: 0.9995\n",
      "Epoch 29/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0051 - f1_m: 0.9984 - val_loss: 0.0090 - val_f1_m: 0.9987\n",
      "Epoch 30/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0057 - f1_m: 0.9982 - val_loss: 0.0053 - val_f1_m: 0.9993\n",
      "Epoch 31/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0045 - f1_m: 0.9986 - val_loss: 0.0029 - val_f1_m: 0.9995\n",
      "Epoch 32/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0043 - f1_m: 0.9986 - val_loss: 0.0024 - val_f1_m: 0.9997\n",
      "Epoch 33/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0040 - f1_m: 0.9987 - val_loss: 0.0023 - val_f1_m: 0.9996\n",
      "Epoch 34/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0038 - f1_m: 0.9988 - val_loss: 0.0024 - val_f1_m: 0.9996\n",
      "Epoch 35/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0047 - f1_m: 0.9986 - val_loss: 0.0046 - val_f1_m: 0.9993\n",
      "Epoch 36/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0036 - f1_m: 0.9989 - val_loss: 0.0039 - val_f1_m: 0.9995\n",
      "Epoch 37/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0038 - f1_m: 0.9988 - val_loss: 0.0028 - val_f1_m: 0.9995\n",
      "Epoch 38/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0037 - f1_m: 0.9988 - val_loss: 0.0044 - val_f1_m: 0.9994\n",
      "Epoch 39/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0039 - f1_m: 0.9988 - val_loss: 0.0012 - val_f1_m: 0.9997\n",
      "Epoch 40/40\n",
      "600512/600512 [==============================] - 16s 26us/sample - loss: 0.0036 - f1_m: 0.9989 - val_loss: 0.0033 - val_f1_m: 0.9995\n",
      "0.9995664788074832\n"
     ]
    }
   ],
   "source": [
    "#Input image\n",
    "input_emb = Input(shape=(256,))\n",
    "input_attr = Input(shape=(16,))\n",
    "\n",
    "encoded_1 = Dense(units=1024, activation='relu')(input_emb)\n",
    "encoded_1 = Dropout(0.1)(encoded_1)\n",
    "encoded_2 = Dense(units=256, activation='relu')(encoded_1)\n",
    "encoded_2 = Dropout(0.1)(encoded_2)\n",
    "encoded_2 = Dense(units=64, activation='relu')(encoded_2)\n",
    "encoded_2 = Dropout(0.1)(encoded_2)\n",
    "encoded = Concatenate()([encoded_2, input_attr])\n",
    "output = Dense(units=16, activation='relu')(encoded)\n",
    "output = Dropout(0.1)(output)\n",
    "output = Dense(units=1, activation='sigmoid', name='autoencoder')(output)\n",
    "\n",
    "autoencoder = Model(inputs = [input_emb, input_attr], outputs = output)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics= [f1_m])\n",
    "\n",
    "history = autoencoder.fit([node_emb, train], y_train,\n",
    "                epochs=40,\n",
    "                batch_size=10000,\n",
    "                validation_data=([omitted_emb, val], y_val))\n",
    "y_pred = autoencoder.predict([omitted_emb, val])[:, 0]\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "score = sklearn.metrics.f1_score(y_val, y_pred)\n",
    "print(score)\n",
    "dico_score['all_features_neural_network_2'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.877, 1.01)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scores = pd.DataFrame.from_dict(dico_score, orient='index')\n",
    "scores.columns = ['score']\n",
    "scores.plot(kind='bar');\n",
    "plt.ylim(0.877, 1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>all_features_neural_network_2</td>\n",
       "      <td>0.999566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>node2vec_alone_neural_network</td>\n",
       "      <td>0.999333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>all_features_neural_network_1</td>\n",
       "      <td>0.998197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_attribut_neural_network</td>\n",
       "      <td>0.974219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_attribut_xgboost</td>\n",
       "      <td>0.972251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_structure_xgboost</td>\n",
       "      <td>0.961866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_attribut_logistic</td>\n",
       "      <td>0.957320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>node_attribut_svm</td>\n",
       "      <td>0.955614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_structure_logistic</td>\n",
       "      <td>0.921562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>node_attribut_xgboost</td>\n",
       "      <td>0.921406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>graph_structure_svm</td>\n",
       "      <td>0.914656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>node_attribut_logistic</td>\n",
       "      <td>0.877750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  score\n",
       "all_features_neural_network_2  0.999566\n",
       "node2vec_alone_neural_network  0.999333\n",
       "all_features_neural_network_1  0.998197\n",
       "graph_attribut_neural_network  0.974219\n",
       "graph_attribut_xgboost         0.972251\n",
       "graph_structure_xgboost        0.961866\n",
       "graph_attribut_logistic        0.957320\n",
       "node_attribut_svm              0.955614\n",
       "graph_structure_logistic       0.921562\n",
       "node_attribut_xgboost          0.921406\n",
       "graph_structure_svm            0.914656\n",
       "node_attribut_logistic         0.877750"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('scores', scores)\n",
    "scores.sort_values(by = 'score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction sur les données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.DataFrame({'node_1':[int(elt[0]) for elt in testing_set], \n",
    "                     'node_2':[int(elt[1]) for elt in testing_set]})\n",
    "emb_test = get_node_emb(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32648/32648 [00:29<00:00, 1119.95it/s]\n"
     ]
    }
   ],
   "source": [
    "pairs_test = [tuple(elt[0 : 2]) for elt in testing_set]\n",
    "with Pool(4) as p: \n",
    "    features_test = p.map(get_features, pairs_test)\n",
    "features_test = np.array(features_test)\n",
    "test_tr = []\n",
    "for elt in tqdm.tqdm(pairs_test) :\n",
    "    test_tr.append(get_graph_feature(elt))\n",
    "test_tr = np.asarray(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train.shape, test.shape)\n",
    "test = np.hstack((test_tr, features_test))\n",
    "test = scaler.transform(test)\n",
    "x_test = np.hstack([emb_test, test])\n",
    "y_test = model.predict(x_test)[:, 0]\n",
    "y_test_1 = np.where(y_test > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600512, 16) (32648, 16)\n"
     ]
    }
   ],
   "source": [
    "## prédiction sur le test\n",
    "test = np.hstack((test_tr, features_test))\n",
    "test = scaler.transform(test)\n",
    "print(train.shape, test.shape)\n",
    "y_test = autoencoder.predict([emb_test, test])[:, 0]\n",
    "y_test_2 = np.where(y_test > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sauvegarde des prédictions\n",
    "predictions_SVM = zip(range(len(testing_set)), y_test_1)\n",
    "with open(\"predictions_1.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('id', 'category'))\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM = zip(range(len(testing_set)), y_test_2)\n",
    "with open(\"predictions_2.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('id', 'category'))\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
