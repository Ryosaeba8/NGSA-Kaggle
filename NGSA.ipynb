{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "###################\n",
    "# random baseline #\n",
    "###################\n",
    "\n",
    "random_predictions = np.random.choice([0, 1], size=len(testing_set))\n",
    "random_predictions = zip(range(len(testing_set)),random_predictions)\n",
    "\n",
    "def write(prediction, filename) :\n",
    "    with open(filename,\"w\") as pred:\n",
    "        csv_out = csv.writer(pred)\n",
    "        csv_out.writerow(('id', 'category'))\n",
    "        for row in prediction:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des fichiers et séparation train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "y = [elt[-1] for elt in training_set]\n",
    "y = np.array(y).astype(int)\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idée rajouter titre à l'abstract\n",
    "corpus = [element[5] + ' ' + element[2].lower() for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('stopwords'); nltk.download('punkt'); nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    result = [stemmer.stem(i) for i in tokens if( (len(i) > 2) &  (i.isalpha()) & (i not in stop_words)) ]\n",
    "    return ' '.join(result)\n",
    "## paralellisation de la lemmatisation des textes\n",
    "from multiprocessing import Pool\n",
    "with Pool(8) as p: \n",
    "    corpus_new = p.map(tokenize, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## création de dictionnaires pour stocker les informations des noeuds\n",
    "dico_node_info = {elt[0]: [elt[1], elt[3].split(\",\"), corpus_new[i].split()] for i, elt in enumerate(node_info)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des mots pour savoir où couper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp = pd.Series((' '.join(corpus_new)).split())\n",
    "tmp = tmp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theori       33359\n",
       "field        21474\n",
       "model        17099\n",
       "gaug         14756\n",
       "string       14042\n",
       "gener        12328\n",
       "solut        10049\n",
       "quantum       9556\n",
       "space         9010\n",
       "use           8609\n",
       "equat         8228\n",
       "algebra       7783\n",
       "function      7696\n",
       "also          7675\n",
       "show          7585\n",
       "discuss       7472\n",
       "result        7435\n",
       "construct     7201\n",
       "term          7141\n",
       "effect        7037\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each paper\n",
    "vectorizer = TfidfVectorizer(decode_error='ignore', smooth_idf=False, stop_words=stop_words,\n",
    "                      encoding='utf-8', min_df = 0, max_df=34000)\n",
    "# each row is a node in the order of node_info\n",
    "corpus_idf = vectorizer.fit_transform(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1001',\n",
       " '2000',\n",
       " 'compactification geometry and duality',\n",
       " 'Paul S. Aspinwall',\n",
       " '',\n",
       " 'these are notes based on lectures given at tasi99 we review the geometry of the moduli space of n 2 theories in four dimensions from the point of view of superstring compactification the cases of a type iia or type iib string compactified on a calabi-yau threefold and the heterotic string compactified on k3xt2 are each considered in detail we pay specific attention to the differences between n 2 theories and n 2 theories the moduli spaces of vector multiplets and the moduli spaces of hypermultiplets are reviewed in the case of hypermultiplets this review is limited by the poor state of our current understanding some peculiarities such as mixed instantons and the non-existence of a universal hypermultiplet are discussed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tuple_to_process) : \n",
    "    ret = []\n",
    "    source = tuple_to_process[0]\n",
    "    target = tuple_to_process[1]\n",
    "    \n",
    "    source_info = dico_node_info[source]\n",
    "    target_info = dico_node_info[target]\n",
    "    \n",
    "    # convert to lowercase and tokenize\n",
    "    source_abstract = source_info[1]\n",
    "    target_abstract = source_info[1]\n",
    "    \n",
    "    source_auth = source_info[2]\n",
    "    target_auth = target_info[2]\n",
    "    \n",
    "    ret.append(len(set(source_abstract).intersection(set(target_abstract))))\n",
    "    ret.append(int(source_info[0]) - int(target_info[0]))\n",
    "    ret.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "    return np.array(ret)\n",
    "import tqdm\n",
    "train_tr = []\n",
    "for elt in tqdm.tqdm(training_set) :\n",
    "    train_tr.append(get_features(elt))\n",
    "train_tr = np.asarray(train_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(553960, 3) (61552, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "id_train, id_val, y_train, y_val = train_test_split(list(range(len(training_set))), \n",
    "                                                    y, test_size=0.1, stratify = y, random_state=10)\n",
    "train = train_tr[id_train]\n",
    "val = train_tr[id_val]\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed: 17.0min remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 18.9min finished\n",
      "/home/jores/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=None, tol=0.0001, verbose=0),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': array([  0.1       ,   0.14384499,   0.20691381,   0.29763514,\n",
       "         0.42813324,   0.61584821,   0.88586679,   1.27427499,\n",
       "         1.83298071,   2.6366509 ,   3.79269019,   5.45559478,\n",
       "         7.8475997 ,  11.28837892,  16.23776739,  23.35721469,\n",
       "        33.59818286,  48.32930239,  69.51927962, 100.        ]),\n",
       "                         'class_weight': ['balanced']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=10)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "classifier = svm.LinearSVC()\n",
    "C = np.logspace(-1, 2, 20)\n",
    "params = dict(C=C, class_weight=['balanced'])\n",
    "clf = GridSearchCV(classifier, params, cv=3, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "clf.fit(train, y_train)\n",
    "import sklearn\n",
    "y_pred = clf.predict(val)\n",
    "print(sklearn.metrics.f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed:    9.8s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7590520385516738\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "C = np.logspace(-1, 2, 20)\n",
    "params = dict(C=C, class_weight=['balanced'], solver=['lbfgs'])\n",
    "clf = GridSearchCV(logistic, params, cv=3, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "clf.fit(train, y_train)\n",
    "y_pred = clf.predict(val)\n",
    "print(sklearn.metrics.f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install node2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame({'node_1':[int(elt[0]) for elt in training_set], \n",
    "                     'node_2':[int(elt[1]) for elt in training_set],\n",
    "                    'link' : [int(elt[2]) for elt in training_set]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.DataFrame({'node_1':[int(elt[0]) for elt in testing_set], \n",
    "                     'node_2':[int(elt[1]) for elt in testing_set]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/335130 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "fb_df_temp = data[data.link == 1]\n",
    "initial_node_count = len(set(fb_df_temp.node_1.unique()).union(set(fb_df_temp.node_2.unique())))\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "for i in tqdm.tqdm(data[data.link == 1].index.values):\n",
    "    break\n",
    "    # remove a node pair and build a new graph\n",
    "    tmp = fb_df_temp.drop(index = i) \n",
    "    all_nodes = set(tmp.node_1.unique()).union(set(tmp.node_2.unique()))\n",
    "    if len(all_nodes) == initial_node_count :\n",
    "        omissible_links_index.append(i)\n",
    "        fb_df_temp = fb_df_temp.drop(index = i)        \n",
    "    else :\n",
    "        pass\n",
    "    if len(omissible_links_index) == 30000 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('omissible.npy', omissible_links_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "omissible_links_index = np.load('omissible.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df_partial = data[data.link == 1].drop(index = omissible_links_index).drop(columns=['link'])\n",
    "G_data = nx.from_pandas_edgelist(fb_df_partial, \"node_1\", \"node_2\", create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 27684/27684 [02:51<00:00, 161.78it/s] \n",
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [1:49:18<00:00, 131.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G_data, dimensions=128, walk_length=100, \n",
    "                    num_walks=50, workers=1, temp_folder='')\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=5, min_count=1)\n",
    "n2w_model.wv.save_word2vec_format('node_embv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "n2w_model = KeyedVectors.load_word2vec_format('node_emb', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_emb(data) :\n",
    "    #node_emb_ = []\n",
    "    node_emb = []\n",
    "    for i, j in zip(data['node_1'], data['node_2']) :\n",
    "        if i in G_data.nodes():\n",
    "            emb_i = n2w_model.wv[str(i)]\n",
    "        else :\n",
    "            emb_i = np.zeros(128)\n",
    "        if j in G_data.nodes():\n",
    "            emb_j = n2w_model.wv[str(j)]\n",
    "        else :\n",
    "            emb_j = np.zeros(128)\n",
    "        node_emb.append(np.vstack((emb_i, emb_j)))\n",
    "        #node_emb_.append(np.hstack((emb_j, emb_i)))\n",
    "        #node_emb.append(emb_i * emb_j)\n",
    "    node_emb = np.asarray(node_emb)\n",
    "    #node_emb_ = np.asarray(node_emb_)\n",
    "    return node_emb #np.vstack((node_emb, node_emb_))\n",
    "node_emb = get_node_emb(data.drop(index = omissible_links_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585512, 2, 128)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#import sklearn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#xtrain, xtest, ytrain, ytest = train_test_split(node_emb, data['link'], \n",
    "#                                                test_size = 0.2, \n",
    "#                                                random_state = 35, stratify = data.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xtrain.shape,xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neural network\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation sur les noeuds omis !!\n",
    "omitted_links = data[data.link == 1].iloc[omissible_links_index]\n",
    "omitted_emb = get_node_emb(omitted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2, 128)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omitted_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data.drop(index = omissible_links_index).link.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585512,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(rnn_cell_size,\n",
    "                              dropout=0.3,\n",
    "                              return_sequences=False,\n",
    "                              return_state=False,\n",
    "                             recurrent_activation='relu',\n",
    "                             name=\"bi_lstm_0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1171024 samples, validate on 60000 samples\n",
      "Epoch 1/10\n",
      "1171024/1171024 [==============================] - 7s 6us/sample - loss: 0.1495 - f1_m: 0.9465 - val_loss: 0.1318 - val_f1_m: 0.9792\n",
      "Epoch 2/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0781 - f1_m: 0.9749 - val_loss: 0.1368 - val_f1_m: 0.9802\n",
      "Epoch 3/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0676 - f1_m: 0.9783 - val_loss: 0.1406 - val_f1_m: 0.9805\n",
      "Epoch 4/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0623 - f1_m: 0.9802 - val_loss: 0.1433 - val_f1_m: 0.9804\n",
      "Epoch 5/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0587 - f1_m: 0.9814 - val_loss: 0.1636 - val_f1_m: 0.9784\n",
      "Epoch 6/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0562 - f1_m: 0.9821 - val_loss: 0.1555 - val_f1_m: 0.9804\n",
      "Epoch 7/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0541 - f1_m: 0.9828 - val_loss: 0.1571 - val_f1_m: 0.9798\n",
      "Epoch 8/10\n",
      "1171024/1171024 [==============================] - 6s 5us/sample - loss: 0.0532 - f1_m: 0.9832 - val_loss: 0.1499 - val_f1_m: 0.9813\n",
      "Epoch 9/10\n",
      "1171024/1171024 [==============================] - 7s 6us/sample - loss: 0.0517 - f1_m: 0.9835 - val_loss: 0.1685 - val_f1_m: 0.9793\n",
      "Epoch 10/10\n",
      "1171024/1171024 [==============================] - 7s 6us/sample - loss: 0.0502 - f1_m: 0.9840 - val_loss: 0.1640 - val_f1_m: 0.9805\n",
      "0.9805454082066094\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "n_classes  = 2\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1, input_shape=(2, 128), activation='relu'))\n",
    "#model.add(Bidirectional(LSTM(32, dropout=0.1,\n",
    "#                             recurrent_activation='relu'), \n",
    "#                        input_shape=(2, 128)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 1024\n",
    "n_epochs = 10\n",
    "history = model.fit(node_emb, y_train,\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(omitted_emb, np.ones(len(omitted_emb))))\n",
    "y_pred = model.predict(omitted_emb)\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "print(sklearn.metrics.f1_score(np.ones(len(omitted_emb)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9799214539519543\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(omitted_emb)\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "print(sklearn.metrics.f1_score(np.ones(len(omitted_emb)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input image\n",
    "input_img= Input(shape=(128,))#  encoded and decoded layer for the autoencoder\n",
    "encoded = Dense(units=32, activation='relu')(input_img)\n",
    "encoded = Dense(units=1, activation='sigmoid')(encoded)\n",
    "\n",
    "decoded = Dense(units=32, activation='relu')(encoded)\n",
    "decoded = Dense(units=128, activation='relu')(decoded)\n",
    "\n",
    "autoencoder=Model(inputs = input_img, [encoded, decoded])#extracting encoder\n",
    "encoder = Model(input_img, encoded)# compiling the autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])# Fitting the noise trained data to the autoencoder \n",
    "autoencoder.fit(X_train_noisy, X_train_noisy,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test_noisy, X_test_noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prédiction finale\n",
    "## ré-entrainement sur le dataset total sans les omis \n",
    "## Flemme puisqu'il faut re-embedder les noeuds, je ferais ça plutard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jores/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "/home/jores/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "emb_test = get_node_emb(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(emb_test)\n",
    "y_test = np.where(y_test >0.5, 1, 0)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM = zip(range(len(testing_set)), y_test)\n",
    "with open(\"improved_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('id', 'category'))\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
