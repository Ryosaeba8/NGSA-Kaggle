{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import igraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "\n",
    "###################\n",
    "# random baseline #\n",
    "###################\n",
    "\n",
    "random_predictions = np.random.choice([0, 1], size=len(testing_set))\n",
    "random_predictions = zip(range(len(testing_set)),random_predictions)\n",
    "\n",
    "def write(prediction, filename) :\n",
    "    with open(filename,\"w\") as pred:\n",
    "        csv_out = csv.writer(pred)\n",
    "        csv_out.writerow(('id', 'category'))\n",
    "        for row in prediction:\n",
    "            csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des fichiers et séparation train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "y = [elt[-1] for elt in training_set]\n",
    "y = np.array(y).astype(int)\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## idée rajouter titre à l'abstract\n",
    "corpus = [element[5] + ' ' + element[2].lower() for element in node_info]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('stopwords'); nltk.download('punkt'); nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def tokenize(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    result = [stemmer.stem(i) for i in tokens if((i.isalpha()) & (i not in stop_words)) ]\n",
    "    return ' '.join(result)\n",
    "## paralellisation de la lemmatisation des textes\n",
    "from multiprocessing import Pool\n",
    "with Pool(8) as p: \n",
    "    corpus_new = p.map(tokenize, corpus)\n",
    "\n",
    "## création d'un dataframe pour stocker les informations des noeuds\n",
    "dico_node_info = {elt[0]: {'pub_year': int(elt[1]), 'authors' : elt[3].split(','),\n",
    "                                'abstract' : corpus_new[i].split()} for i, elt in enumerate(node_info)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppression volontaire de quelques liens pour valider le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/335130 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "data_ = pd.DataFrame({'node_1':[int(elt[0]) for elt in training_set], \n",
    "                     'node_2':[int(elt[1]) for elt in training_set],\n",
    "                    'link' : [int(elt[2]) for elt in training_set]})\n",
    "#fb_df_temp = data_[data_.link == 1]\n",
    "#fb_df_temp.link = 1 - fb_df_temp.link\n",
    "#fb_df_temp.columns = ['node_2', 'node_1', 'link']\n",
    "\n",
    "#data_ = data_.append(fb_df_temp)\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "fb_df_temp = data_[data_.link == 1]\n",
    "initial_node_count = len(set(fb_df_temp.node_1.unique()).union(set(fb_df_temp.node_2.unique())))\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "for i in tqdm.tqdm(data_[data_.link == 1].index.values):\n",
    "    break\n",
    "    # remove a node pair and build a new graph\n",
    "    tmp = fb_df_temp.drop(index = i) \n",
    "    all_nodes = set(tmp.node_1.unique()).union(set(tmp.node_2.unique()))\n",
    "    if len(all_nodes) == initial_node_count :\n",
    "        omissible_links_index.append(i)\n",
    "        fb_df_temp = fb_df_temp.drop(index = i)        \n",
    "    else :\n",
    "        pass\n",
    "    if len(omissible_links_index) == 30000 :\n",
    "        break\n",
    "#np.save('omissible.npy', omissible_links_index)\n",
    "omissible_links_index = np.load('omissible.npy')\n",
    "fb_df_partial = data_[data_.link == 1].drop(index = omissible_links_index).drop(columns=['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = set(fb_df_temp.node_1.unique()).union(set(fb_df_temp.node_2.unique()))\n",
    "nodes_bizz = [node for node in dico_node_info.keys() if int(node) not in all_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_data = nx.from_pandas_edgelist(fb_df_partial, \"node_1\", \"node_2\", create_using=nx.DiGraph())\n",
    "G_data.add_nodes_from(nodes_bizz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d4a3504c0a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'G_data' is not defined"
     ]
    }
   ],
   "source": [
    "len(G_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = [tuple(elt[:2]) for elt in np.array(training_set).astype(int)] +\\\n",
    "            [tuple(elt[:2]) for elt in np.array(testing_set).astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with attributes of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.Series(''.join(corpus_new).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8705,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp > 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27770, 8868)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TF-IDF pour la similarité entre abstract + titre\n",
    "vectorizer = TfidfVectorizer(decode_error='ignore', smooth_idf=False, stop_words=stop_words,\n",
    "                      encoding='utf-8', min_df = 2, max_df=5000)\n",
    "# each row is a node in the order of node_info\n",
    "corpus_idf = vectorizer.fit_transform(corpus_new)\n",
    "corpus_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Etude du delta entre les années\n",
    "tmp_ = pd.DataFrame.from_dict(dico_node_info).T[['pub_year', 'authors']]\n",
    "tmp_['node_1'] = tmp_.index.astype(int)\n",
    "tmp = pd.merge(tmp_, data_, on='node_1')\n",
    "tmp = tmp.rename(columns = {\"pub_year\":\"pub_year_1\"})\n",
    "tmp_ = tmp_.rename(columns = {\"node_1\":\"node_2\"})\n",
    "tmp = pd.merge(tmp_, tmp, on='node_2')\n",
    "tmp = tmp.rename(columns = {\"pub_year\":\"pub_year_2\"})\n",
    "tmp['diff_year'] = tmp.pub_year_1 - tmp.pub_year_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.35963022172794"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 98% des delta_year < 0 sont pas connectés \n",
    "100*( 1 - len(tmp[(tmp.diff_year < 0 ) & (tmp.link == 1)])/(tmp.diff_year < 0 ).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_year_2</th>\n",
       "      <th>authors_x</th>\n",
       "      <th>node_2</th>\n",
       "      <th>pub_year_1</th>\n",
       "      <th>authors_y</th>\n",
       "      <th>node_1</th>\n",
       "      <th>link</th>\n",
       "      <th>diff_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Csaba Csaki,  Joshua Erlich,  Timothy J. Holl...</td>\n",
       "      <td>1033</td>\n",
       "      <td>1999</td>\n",
       "      <td>[O. DeWolfe,  D.Z. Freedman,  S.S. Gubser,  , ...</td>\n",
       "      <td>9909134</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>2000</td>\n",
       "      <td>[O. Kenneth,  S. Nussinov]</td>\n",
       "      <td>1045</td>\n",
       "      <td>1999</td>\n",
       "      <td>[O. Kenneth,  S. Nussinov]</td>\n",
       "      <td>9912291</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>2001</td>\n",
       "      <td>[Kenji Suzuki]</td>\n",
       "      <td>1057</td>\n",
       "      <td>2000</td>\n",
       "      <td>[M. Caselle]</td>\n",
       "      <td>3119</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677</td>\n",
       "      <td>2002</td>\n",
       "      <td>[A. Sagnotti]</td>\n",
       "      <td>1077</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Sunil Mukhi,  Nemani V. Suryanarayana (Tata I...</td>\n",
       "      <td>3219</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1679</td>\n",
       "      <td>2002</td>\n",
       "      <td>[A. Sagnotti]</td>\n",
       "      <td>1077</td>\n",
       "      <td>2000</td>\n",
       "      <td>[C. Angelantonj,  I. Antoniadis,  E. Dudas,  A...</td>\n",
       "      <td>7090</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pub_year_2                                          authors_x  node_2  \\\n",
       "768        2000  [Csaba Csaki,  Joshua Erlich,  Timothy J. Holl...    1033   \n",
       "1004       2000                         [O. Kenneth,  S. Nussinov]    1045   \n",
       "1307       2001                                     [Kenji Suzuki]    1057   \n",
       "1677       2002                                      [A. Sagnotti]    1077   \n",
       "1679       2002                                      [A. Sagnotti]    1077   \n",
       "\n",
       "     pub_year_1                                          authors_y   node_1  \\\n",
       "768        1999  [O. DeWolfe,  D.Z. Freedman,  S.S. Gubser,  , ...  9909134   \n",
       "1004       1999                         [O. Kenneth,  S. Nussinov]  9912291   \n",
       "1307       2000                                       [M. Caselle]     3119   \n",
       "1677       2000  [Sunil Mukhi,  Nemani V. Suryanarayana (Tata I...     3219   \n",
       "1679       2000  [C. Angelantonj,  I. Antoniadis,  E. Dudas,  A...     7090   \n",
       "\n",
       "      link diff_year  \n",
       "768      1        -1  \n",
       "1004     1        -1  \n",
       "1307     1        -1  \n",
       "1677     1        -2  \n",
       "1679     1        -2  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[(tmp.diff_year < 0 ) & (tmp.link == 1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44045518e-04, 3.60113796e-05, 1.47016829e-05, 5.18727784e-09,\n",
       "       9.06481076e-11, 3.12322908e-12, 2.00000000e+00, 6.66666667e-01,\n",
       "       4.00000000e-01, 1.82047845e+00, 1.00000000e+00, 1.00000000e+00])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_graph_feature(('9912291', '1045'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(tuple_to_process) : \n",
    "    ret = []\n",
    "    source = tuple_to_process[0]\n",
    "    target = tuple_to_process[1]\n",
    "    \n",
    "    source_info = dico_node_info[source]\n",
    "    target_info = dico_node_info[target]\n",
    "    \n",
    "    source_abstract = set(source_info['abstract'])\n",
    "    target_abstract = set(target_info['abstract'])\n",
    "    \n",
    "    source_auth = set(source_info['authors'])\n",
    "    target_auth = set(target_info['authors'])\n",
    "    \n",
    "    ## delta entre dates de publication\n",
    "    delta_year = int(source_info['pub_year']) - int(target_info['pub_year'])\n",
    "    if delta_year < 0 :\n",
    "        delta_year = 0\n",
    "    else :\n",
    "        delta_year = 1\n",
    "    \n",
    "    ## cosine similiarity et intersection auteurs abstracts\n",
    "    common_words = len(source_abstract.intersection(target_abstract))/(len(source_abstract.union(target_abstract)))\n",
    "    common_auths = len(source_auth.intersection(target_auth))/(len(source_auth.union(target_auth)))\n",
    "    \n",
    "    cosine_sim = corpus_idf[IDs.index(source)].dot( \n",
    "                        corpus_idf[IDs.index(target)].T).toarray()[0][0] ## les vecteurs sont déjà normés \n",
    "    \n",
    "    ret.append(common_words)\n",
    "    ret.append(cosine_sim)\n",
    "    ret.append(delta_year)\n",
    "    ret.append(common_auths)\n",
    "    \n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim = corpus_idf[IDs.index(target)].dot(corpus_idf[IDs.index(source)].T).toarray()[0, 0]\n",
    "#source, target = pairs_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features liées à la structure du graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(G, ebunch = None):\n",
    "    u = ebunch[0]\n",
    "    v = ebunch[1]\n",
    "    if str(u) in nodes_bizz :\n",
    "        u = str(u)\n",
    "    if str(v) in nodes_bizz :\n",
    "        v = str(v)\n",
    "    common_neighbors =  [w for w in G[u] if w in G[v] and w not in (u, v)]\n",
    "    return common_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 648160/648160 [00:25<00:00, 25922.85it/s]\n"
     ]
    }
   ],
   "source": [
    "dico_common_neighbors = {}\n",
    "for elt in tqdm.tqdm(all_pairs) :\n",
    "    dico_common_neighbors[elt] = common_neighbors(G_data, elt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resource_allocation_index(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        return sum(1 / G.degree(w) for w in dico_common_neighbors.get(key_, []))\n",
    "    return predict(ebunch[0], ebunch[1])\n",
    "def jaccard_coefficient(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        cnbors = list(dico_common_neighbors.get(key_,[]))\n",
    "        union_size = len(set(G[u]) | set(G[v]))\n",
    "        if union_size == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return len(cnbors) / union_size\n",
    "    \n",
    "    return predict(ebunch[0], ebunch[1])\n",
    "import math\n",
    "def adamic_adar_index(G, ebunch=None):\n",
    "    if ebunch is None:\n",
    "        ebunch = nx.non_edges(G)\n",
    "  \n",
    "    def predict(u, v):\n",
    "        key_ =  (u, v)\n",
    "        return sum(1 / math.log(G.degree(w)) for w in dico_common_neighbors.get(key_, []))\n",
    "  \n",
    "    return predict(ebunch[0], ebunch[1])\n",
    "def resource_allocation_CNI(G, ebunch = None):\n",
    "    u = ebunch[0]\n",
    "    v = ebunch[1]\n",
    "    key_ =  (u, v)\n",
    "    common_neighbors = dico_common_neighbors.get(key_,[])\n",
    "    first = sum([1/(len(set(G.neighbors(i)))) for i in common_neighbors if (len(set(G.neighbors(i)))) != 0])\n",
    "\n",
    "    #neighbor_x = G.neighbors(u)\n",
    "    #neighbor_y = G.neighbors(v)\n",
    "\n",
    "    #second = sum([(1/(len(set(G.neighbors(cross[0]))))) - (1/(len(set(G.neighbors(cross[1]))))) for cross in list(itertools.product(list(neighbor_x), list(neighbor_y))) \n",
    "    #            if cross in list(G.edges)])\n",
    "\n",
    "    return first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_degree_centrality = nx.out_degree_centrality(G_data)\n",
    "in_degree_centrality = nx.in_degree_centrality(G_data)\n",
    "page_rank = nx.pagerank_scipy(G_data)\n",
    "hub_score, authority_score = nx.hits(G_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_graph_feature(tuple_to_process) :\n",
    "    ret = []\n",
    "    source = tuple_to_process[0]\n",
    "    target = tuple_to_process[1]\n",
    "    if source not in nodes_bizz :\n",
    "        source = int(source)\n",
    "    if target not in nodes_bizz :\n",
    "        target = int(target)\n",
    "    ebunch = (source, target)    \n",
    "    ret.append(out_degree_centrality[source])\n",
    "    ret.append(in_degree_centrality[target])\n",
    "    ret.append(page_rank[target])\n",
    "    ret.append(out_degree_centrality[source] * in_degree_centrality[target])\n",
    "    ret.append(hub_score[source])\n",
    "    ret.append(authority_score[target])\n",
    "    ret.append(len(dico_common_neighbors.get(ebunch, [])))\n",
    "    ret.append(resource_allocation_index(G_data, ebunch))\n",
    "    ret.append(jaccard_coefficient(G_data, ebunch))\n",
    "    ret.append(adamic_adar_index(G_data, ebunch))\n",
    "    ret.append(resource_allocation_CNI(G_data, ebunch))\n",
    "    #try :\n",
    "    #    ret.append(nx.shortest_path_length(G_data, source, target))\n",
    "    #except :\n",
    "    #    ret.append(-1.)\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "585512"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_train = [tuple(elt[0 : 2]) for i, elt in enumerate(training_set) if i not in omissible_links_index]\n",
    "y_train = [int(elt[-1]) for i, elt in enumerate(training_set) if i not in omissible_links_index]\n",
    "len(pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(6) as p: \n",
    "    features_nodes = p.map(get_features, pairs_train)\n",
    "features_nodes = np.array(features_nodes)\n",
    "\n",
    "with Pool(6) as p: \n",
    "    train_tr = p.map(get_graph_feature, pairs_train )\n",
    "train_tr = np.asarray(train_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585512, 4)\n",
      "(585512, 12)\n"
     ]
    }
   ],
   "source": [
    "print(features_nodes.shape)\n",
    "print(train_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_val = [tuple(elt[0 : 2]) for i, elt in enumerate(training_set) if i in omissible_links_index]\n",
    "y_val = np.array([int(elt[-1]) for i, elt in enumerate(training_set) if i in omissible_links_index])\n",
    "len(pairs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(4) as p: \n",
    "    features_val = p.map(get_features, pairs_val)\n",
    "features_val = np.array(features_val)\n",
    "#val_tr = []\n",
    "with Pool(6) as p: \n",
    "    val_tr = p.map(get_graph_feature, pairs_val)\n",
    "val_tr = np.asarray(val_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn\n",
    "def train_logistic(train, val, c_inf=-1, c_up=2, n=20) : \n",
    "    logistic = linear_model.LogisticRegression()\n",
    "    C = np.logspace(c_inf, c_up, n)\n",
    "    params = dict(C=C, class_weight=['balanced'], solver=['lbfgs'])\n",
    "    clf = GridSearchCV(logistic, params, cv=3, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "    clf.fit(train, y_train)\n",
    "    y_pred = clf.predict(val)\n",
    "    print(sklearn.metrics.f1_score(y_val, y_pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize basic SVM\n",
    "def train_svm(train, val) : \n",
    "    classifier = svm.LinearSVC()\n",
    "    C = np.logspace(0, 1, 2)\n",
    "    params = dict(C=C, class_weight=['balanced'])\n",
    "    clf = GridSearchCV(classifier, params, cv=3, verbose=10, n_jobs = -1,  scoring='f1')\n",
    "    clf.fit(train, y_train)\n",
    "    y_pred = clf.predict(val)\n",
    "    print(sklearn.metrics.f1_score(y_val, y_pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## f-1 score\n",
    "import tensorflow.keras.backend as K\n",
    "import sklearn\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec uniquement les features graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585512, 11) (30000, 11)\n"
     ]
    }
   ],
   "source": [
    "train = scaler.fit_transform(train_tr)\n",
    "val = scaler.transform(val_tr)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.1s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed:  1.3min remaining:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906904718527965\n"
     ]
    }
   ],
   "source": [
    "clf = train_logistic(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.2742749857031335, 'class_weight': 'balanced', 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:  2.6min remaining: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:  2.7min remaining:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:  2.7min remaining:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  2.9min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:  5.1min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  5.1min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7223983405737544\n"
     ]
    }
   ],
   "source": [
    "#train_svm(train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec uniquement les attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585512, 4) (30000, 4)\n"
     ]
    }
   ],
   "source": [
    "train = features_nodes #scaler.fit_transform(features_nodes)\n",
    "val = features_val #scaler.transform(features_val)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:   28.7s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   29.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9218716308488464\n"
     ]
    }
   ],
   "source": [
    "clf = train_logistic(train, val, c_inf=-3, c_up=-1, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.0041753189365604, 'class_weight': 'balanced', 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:   57.2s remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   6 | elapsed:  1.0min remaining:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   6 | elapsed:  1.2min remaining:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835808495170838\n"
     ]
    }
   ],
   "source": [
    "#train_svm(train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En concatenant les deux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585512, 15) (30000, 15)\n"
     ]
    }
   ],
   "source": [
    "train = np.hstack((train_tr, features_nodes))\n",
    "val = np.hstack((val_tr, features_val))\n",
    "train = scaler.fit_transform(train)\n",
    "val = scaler.transform(val)\n",
    "print(train.shape, val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   46.0s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   58.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed:  1.4min remaining:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585864546811539\n"
     ]
    }
   ],
   "source": [
    "clf = train_logistic(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100.0, 'class_weight': 'balanced', 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9481434732302514\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {'subsample': 0.7,\n",
    " 'reg_lambda': 1,\n",
    " 'reg_alpha': 0.1,\n",
    " 'n_estimators': 200,\n",
    " 'min_samples_split': 3,\n",
    " 'min_samples_leaf': 3,\n",
    " 'min_child_weight': 7,\n",
    " 'max_depth': 30,\n",
    " 'learning_rate': 0.1,\n",
    " 'gamma': 0.1,\n",
    " 'criterion': 'entropy',\n",
    " 'colsample_bytree': 1,\n",
    " 'colsample_bylevel': 0.7,\n",
    "         'n_jobs':-1}\n",
    "clf = XGBClassifier()\n",
    "clf.set_params(**params)\n",
    "clf.fit(train, y_train)\n",
    "y_pred = clf.predict(val)\n",
    "print(sklearn.metrics.f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 585512 samples, validate on 30000 samples\n",
      "Epoch 1/30\n",
      "585512/585512 [==============================] - 1s 2us/sample - loss: 0.2761 - f1_m: 0.8979 - val_loss: 0.1646 - val_f1_m: 0.9626\n",
      "Epoch 2/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1750 - f1_m: 0.9393 - val_loss: 0.1449 - val_f1_m: 0.9680\n",
      "Epoch 3/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1658 - f1_m: 0.9426 - val_loss: 0.1482 - val_f1_m: 0.9688\n",
      "Epoch 4/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1616 - f1_m: 0.9439 - val_loss: 0.1510 - val_f1_m: 0.9686\n",
      "Epoch 5/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1585 - f1_m: 0.9449 - val_loss: 0.1502 - val_f1_m: 0.9687\n",
      "Epoch 6/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1569 - f1_m: 0.9454 - val_loss: 0.1475 - val_f1_m: 0.9694\n",
      "Epoch 7/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1548 - f1_m: 0.9460 - val_loss: 0.1428 - val_f1_m: 0.9706\n",
      "Epoch 8/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1535 - f1_m: 0.9465 - val_loss: 0.1408 - val_f1_m: 0.9704\n",
      "Epoch 9/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1528 - f1_m: 0.9465 - val_loss: 0.1405 - val_f1_m: 0.9705\n",
      "Epoch 10/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1511 - f1_m: 0.9471 - val_loss: 0.1384 - val_f1_m: 0.9711\n",
      "Epoch 11/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1500 - f1_m: 0.9474 - val_loss: 0.1412 - val_f1_m: 0.9706\n",
      "Epoch 12/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1494 - f1_m: 0.9475 - val_loss: 0.1481 - val_f1_m: 0.9691\n",
      "Epoch 13/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1486 - f1_m: 0.9477 - val_loss: 0.1333 - val_f1_m: 0.9721\n",
      "Epoch 14/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1482 - f1_m: 0.9478 - val_loss: 0.1412 - val_f1_m: 0.9704\n",
      "Epoch 15/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1477 - f1_m: 0.9479 - val_loss: 0.1449 - val_f1_m: 0.9697\n",
      "Epoch 16/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1470 - f1_m: 0.9482 - val_loss: 0.1463 - val_f1_m: 0.9688\n",
      "Epoch 17/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1470 - f1_m: 0.9483 - val_loss: 0.1379 - val_f1_m: 0.9708\n",
      "Epoch 18/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1462 - f1_m: 0.9483 - val_loss: 0.1389 - val_f1_m: 0.9704\n",
      "Epoch 19/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1460 - f1_m: 0.9484 - val_loss: 0.1445 - val_f1_m: 0.9692\n",
      "Epoch 20/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1454 - f1_m: 0.9485 - val_loss: 0.1432 - val_f1_m: 0.9689\n",
      "Epoch 21/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1456 - f1_m: 0.9486 - val_loss: 0.1436 - val_f1_m: 0.9698\n",
      "Epoch 22/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1452 - f1_m: 0.9487 - val_loss: 0.1482 - val_f1_m: 0.9685\n",
      "Epoch 23/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1448 - f1_m: 0.9488 - val_loss: 0.1451 - val_f1_m: 0.9689\n",
      "Epoch 24/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1447 - f1_m: 0.9488 - val_loss: 0.1347 - val_f1_m: 0.9715\n",
      "Epoch 25/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1443 - f1_m: 0.9488 - val_loss: 0.1402 - val_f1_m: 0.9705\n",
      "Epoch 26/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1436 - f1_m: 0.9490 - val_loss: 0.1395 - val_f1_m: 0.9702\n",
      "Epoch 27/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1435 - f1_m: 0.9491 - val_loss: 0.1435 - val_f1_m: 0.9690\n",
      "Epoch 28/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1432 - f1_m: 0.9490 - val_loss: 0.1503 - val_f1_m: 0.9673\n",
      "Epoch 29/30\n",
      "585512/585512 [==============================] - 1s 1us/sample - loss: 0.1432 - f1_m: 0.9491 - val_loss: 0.1383 - val_f1_m: 0.9705\n",
      "Epoch 30/30\n",
      "585512/585512 [==============================] - 1s 2us/sample - loss: 0.1431 - f1_m: 0.9492 - val_loss: 0.1481 - val_f1_m: 0.9688\n",
      "0.9687000464084979\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input, BatchNormalization\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 2048\n",
    "n_epochs = 30\n",
    "history = model.fit(train, np.array(y_train),\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(val, np.array(y_val)))\n",
    "y_pred = model.predict(val)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "print(sklearn.metrics.f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 27770/27770 [02:38<00:00, 175.69it/s] \n",
      "Generating walks (CPU: 1): 100%|██████████| 30/30 [28:46<00:00, 57.55s/it]\n"
     ]
    }
   ],
   "source": [
    "G_data = nx.from_pandas_edgelist(fb_df_partial, \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "G_data.add_nodes_from(nodes_bizz)\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G_data, dimensions=128, walk_length=50, \n",
    "                    num_walks=30, workers=1, temp_folder='')\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=5, min_count=1)\n",
    "n2w_model.wv.save_word2vec_format('node_emb_partiel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities:   1%|          | 139/27770 [00:03<10:18, 44.70it/s]"
     ]
    }
   ],
   "source": [
    "edges = data_[data_.link == 1]\n",
    "G_data = nx.from_pandas_edgelist(edges, \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "G_data.add_nodes_from(nodes_bizz)\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G_data, dimensions=128, walk_length=50, \n",
    "                    num_walks=50, workers=4, temp_folder='')\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=8, min_count=1)\n",
    "n2w_model.wv.save_word2vec_format('node_emb_total_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node embeddings + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing transition probabilities:   0%|          | 0/27770 [00:00<?, ?it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|          | 58/27770 [00:00<00:47, 579.37it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|          | 169/27770 [00:00<00:40, 675.65it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|          | 217/27770 [00:00<00:46, 594.96it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|          | 267/27770 [00:00<00:49, 557.49it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|          | 314/27770 [00:00<02:18, 198.74it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|▏         | 349/27770 [00:01<02:23, 190.92it/s]\u001b[A\n",
      "Computing transition probabilities:   1%|▏         | 383/27770 [00:01<02:05, 218.16it/s]\u001b[A\n",
      "Computing transition probabilities:   2%|▏         | 417/27770 [00:01<01:52, 244.18it/s]\u001b[A\n",
      "Computing transition probabilities:   2%|▏         | 488/27770 [00:01<01:29, 303.15it/s]\u001b[A\n",
      "Computing transition probabilities:   2%|▏         | 531/27770 [00:01<01:27, 310.64it/s]\u001b[A\n",
      "Computing transition probabilities:   2%|▏         | 595/27770 [00:01<01:14, 367.10it/s]\u001b[A\n",
      "Computing transition probabilities:   2%|▏         | 655/27770 [00:01<01:05, 413.76it/s]\u001b[A\n",
      "Computing transition probabilities:   3%|▎         | 730/27770 [00:01<00:56, 476.86it/s]\u001b[A\n",
      "Computing transition probabilities:   3%|▎         | 794/27770 [00:02<00:52, 514.33it/s]\u001b[A\n",
      "Computing transition probabilities:   3%|▎         | 859/27770 [00:02<00:49, 548.60it/s]\u001b[A\n",
      "Computing transition probabilities:   3%|▎         | 921/27770 [00:02<00:47, 565.65it/s]\u001b[A\n",
      "Computing transition probabilities:   4%|▎         | 994/27770 [00:02<00:44, 603.31it/s]\u001b[A\n",
      "Computing transition probabilities:   4%|▍         | 1059/27770 [00:02<00:45, 587.25it/s]\u001b[A\n",
      "Computing transition probabilities:   4%|▍         | 1134/27770 [00:02<00:42, 628.06it/s]\u001b[A\n",
      "Computing transition probabilities:   4%|▍         | 1200/27770 [00:02<00:44, 591.78it/s]\u001b[A\n",
      "Computing transition probabilities:   5%|▍         | 1276/27770 [00:02<00:42, 630.03it/s]\u001b[A\n",
      "Computing transition probabilities:   5%|▍         | 1342/27770 [00:02<00:44, 593.05it/s]\u001b[A\n",
      "Computing transition probabilities:   5%|▌         | 1404/27770 [00:03<00:44, 591.83it/s]\u001b[A\n",
      "Computing transition probabilities:   5%|▌         | 1479/27770 [00:03<00:41, 629.13it/s]\u001b[A\n",
      "Computing transition probabilities:   6%|▌         | 1544/27770 [00:03<00:42, 622.38it/s]\u001b[A\n",
      "Computing transition probabilities:   6%|▌         | 1608/27770 [00:03<00:42, 610.19it/s]\u001b[A\n",
      "Computing transition probabilities:   6%|▌         | 1671/27770 [00:03<00:42, 613.02it/s]\u001b[A\n",
      "Computing transition probabilities:   6%|▌         | 1733/27770 [00:03<00:43, 600.96it/s]\u001b[A\n",
      "Computing transition probabilities:   6%|▋         | 1801/27770 [00:03<00:41, 622.03it/s]\u001b[A\n",
      "Computing transition probabilities:   7%|▋         | 1878/27770 [00:03<00:39, 658.86it/s]\u001b[A\n",
      "Computing transition probabilities:   7%|▋         | 1945/27770 [00:03<00:40, 640.86it/s]\u001b[A\n",
      "Computing transition probabilities:   7%|▋         | 2023/27770 [00:03<00:38, 671.68it/s]\u001b[A\n",
      "Computing transition probabilities:   8%|▊         | 2092/27770 [00:04<00:40, 640.66it/s]\u001b[A\n",
      "Computing transition probabilities:   8%|▊         | 2179/27770 [00:04<00:36, 695.56it/s]\u001b[A\n",
      "Computing transition probabilities:   8%|▊         | 2251/27770 [00:04<00:37, 674.31it/s]\u001b[A\n",
      "Computing transition probabilities:   8%|▊         | 2321/27770 [00:04<00:37, 672.28it/s]\u001b[A\n",
      "Computing transition probabilities:   9%|▊         | 2390/27770 [00:04<00:37, 675.14it/s]\u001b[A\n",
      "Computing transition probabilities:   9%|▉         | 2473/27770 [00:04<00:35, 714.08it/s]\u001b[A\n",
      "Computing transition probabilities:   9%|▉         | 2552/27770 [00:04<00:34, 735.22it/s]\u001b[A\n",
      "Computing transition probabilities:   9%|▉         | 2638/27770 [00:04<00:32, 768.34it/s]\u001b[A\n",
      "Computing transition probabilities:  10%|▉         | 2716/27770 [00:04<00:34, 731.19it/s]\u001b[A\n",
      "Computing transition probabilities:  10%|█         | 2791/27770 [00:05<00:35, 698.08it/s]\u001b[A\n",
      "Computing transition probabilities:  10%|█         | 2878/27770 [00:05<00:33, 740.85it/s]\u001b[A\n",
      "Computing transition probabilities:  11%|█         | 2954/27770 [00:05<00:34, 712.20it/s]\u001b[A\n",
      "Computing transition probabilities:  11%|█         | 3030/27770 [00:05<00:34, 725.25it/s]\u001b[A\n",
      "Computing transition probabilities:  11%|█         | 3111/27770 [00:05<00:32, 748.69it/s]\u001b[A\n",
      "Computing transition probabilities:  11%|█▏        | 3187/27770 [00:05<00:37, 654.60it/s]\u001b[A\n",
      "Computing transition probabilities:  12%|█▏        | 3257/27770 [00:05<00:36, 662.74it/s]\u001b[A\n",
      "Computing transition probabilities:  12%|█▏        | 3326/27770 [00:05<00:36, 664.90it/s]\u001b[A\n",
      "Computing transition probabilities:  12%|█▏        | 3411/27770 [00:05<00:34, 708.01it/s]\u001b[A\n",
      "Computing transition probabilities:  13%|█▎        | 3484/27770 [00:06<00:34, 697.30it/s]\u001b[A\n",
      "Computing transition probabilities:  13%|█▎        | 3560/27770 [00:06<00:34, 707.33it/s]\u001b[A\n",
      "Computing transition probabilities:  13%|█▎        | 3642/27770 [00:06<00:32, 737.46it/s]\u001b[A\n",
      "Computing transition probabilities:  13%|█▎        | 3727/27770 [00:06<00:31, 766.79it/s]\u001b[A\n",
      "Computing transition probabilities:  14%|█▎        | 3805/27770 [00:06<00:32, 747.49it/s]\u001b[A\n",
      "Computing transition probabilities:  14%|█▍        | 3888/27770 [00:06<00:31, 767.05it/s]\u001b[A\n",
      "Computing transition probabilities:  14%|█▍        | 3966/27770 [00:06<00:31, 752.89it/s]\u001b[A\n",
      "Computing transition probabilities:  15%|█▍        | 4069/27770 [00:06<00:29, 816.85it/s]\u001b[A\n",
      "Computing transition probabilities:  15%|█▍        | 4154/27770 [00:06<00:28, 826.33it/s]\u001b[A\n",
      "Computing transition probabilities:  15%|█▌        | 4252/27770 [00:06<00:27, 866.55it/s]\u001b[A\n",
      "Computing transition probabilities:  16%|█▌        | 4341/27770 [00:07<00:27, 837.84it/s]\u001b[A\n",
      "Computing transition probabilities:  16%|█▌        | 4427/27770 [00:07<00:29, 798.99it/s]\u001b[A\n",
      "Computing transition probabilities:  16%|█▌        | 4509/27770 [00:07<00:31, 746.03it/s]\u001b[A\n",
      "Computing transition probabilities:  17%|█▋        | 4586/27770 [00:07<00:32, 712.82it/s]\u001b[A\n",
      "Computing transition probabilities:  17%|█▋        | 4659/27770 [00:07<00:32, 710.47it/s]\u001b[A\n",
      "Computing transition probabilities:  17%|█▋        | 4747/27770 [00:07<00:30, 753.98it/s]\u001b[A\n",
      "Computing transition probabilities:  17%|█▋        | 4825/27770 [00:07<00:30, 760.08it/s]\u001b[A\n",
      "Computing transition probabilities:  18%|█▊        | 4920/27770 [00:07<00:28, 805.43it/s]\u001b[A\n",
      "Computing transition probabilities:  18%|█▊        | 5002/27770 [00:07<00:29, 775.28it/s]\u001b[A\n",
      "Computing transition probabilities:  18%|█▊        | 5084/27770 [00:08<00:28, 786.33it/s]\u001b[A\n",
      "Computing transition probabilities:  19%|█▊        | 5175/27770 [00:08<00:27, 818.52it/s]\u001b[A\n",
      "Computing transition probabilities:  19%|█▉        | 5268/27770 [00:08<00:26, 845.24it/s]\u001b[A\n",
      "Computing transition probabilities:  19%|█▉        | 5357/27770 [00:08<00:26, 857.10it/s]\u001b[A\n",
      "Computing transition probabilities:  20%|█▉        | 5444/27770 [00:08<00:26, 829.46it/s]\u001b[A\n",
      "Computing transition probabilities:  20%|█▉        | 5543/27770 [00:08<00:25, 867.32it/s]\u001b[A\n",
      "Computing transition probabilities:  20%|██        | 5631/27770 [00:08<00:25, 859.29it/s]\u001b[A\n",
      "Computing transition probabilities:  21%|██        | 5735/27770 [00:08<00:24, 906.48it/s]\u001b[A\n",
      "Computing transition probabilities:  21%|██        | 5827/27770 [00:08<00:24, 906.31it/s]\u001b[A\n",
      "Computing transition probabilities:  21%|██▏       | 5928/27770 [00:08<00:23, 933.16it/s]\u001b[A\n",
      "Computing transition probabilities:  22%|██▏       | 6038/27770 [00:09<00:22, 972.31it/s]\u001b[A\n",
      "Computing transition probabilities:  22%|██▏       | 6137/27770 [00:09<00:23, 931.66it/s]\u001b[A\n",
      "Computing transition probabilities:  22%|██▏       | 6232/27770 [00:09<00:24, 866.97it/s]\u001b[A\n",
      "Computing transition probabilities:  23%|██▎       | 6325/27770 [00:09<00:24, 883.95it/s]\u001b[A\n",
      "Computing transition probabilities:  23%|██▎       | 6421/27770 [00:09<00:23, 903.56it/s]\u001b[A\n",
      "Computing transition probabilities:  23%|██▎       | 6513/27770 [00:09<00:23, 893.17it/s]\u001b[A\n",
      "Computing transition probabilities:  24%|██▍       | 6604/27770 [00:09<00:24, 849.70it/s]\u001b[A\n",
      "Computing transition probabilities:  24%|██▍       | 6709/27770 [00:09<00:23, 900.57it/s]\u001b[A\n",
      "Computing transition probabilities:  25%|██▍       | 6816/27770 [00:09<00:22, 945.05it/s]\u001b[A\n",
      "Computing transition probabilities:  25%|██▍       | 6922/27770 [00:10<00:21, 976.22it/s]\u001b[A\n",
      "Computing transition probabilities:  25%|██▌       | 7022/27770 [00:10<00:21, 966.44it/s]\u001b[A\n",
      "Computing transition probabilities:  26%|██▌       | 7156/27770 [00:10<00:19, 1053.38it/s]\u001b[A\n",
      "Computing transition probabilities:  26%|██▌       | 7265/27770 [00:10<00:21, 946.01it/s] \u001b[A\n",
      "Computing transition probabilities:  27%|██▋       | 7367/27770 [00:10<00:21, 966.38it/s]\u001b[A\n",
      "Computing transition probabilities:  27%|██▋       | 7476/27770 [00:10<00:20, 997.19it/s]\u001b[A\n",
      "Computing transition probabilities:  27%|██▋       | 7579/27770 [00:10<00:21, 940.83it/s]\u001b[A\n",
      "Computing transition probabilities:  28%|██▊       | 7676/27770 [00:10<00:21, 915.68it/s]\u001b[A\n",
      "Computing transition probabilities:  28%|██▊       | 7796/27770 [00:10<00:20, 978.91it/s]\u001b[A\n",
      "Computing transition probabilities:  28%|██▊       | 7897/27770 [00:11<00:20, 979.02it/s]\u001b[A\n",
      "Computing transition probabilities:  29%|██▉       | 7998/27770 [00:11<00:20, 986.07it/s]\u001b[A\n",
      "Computing transition probabilities:  29%|██▉       | 8098/27770 [00:11<00:21, 925.76it/s]\u001b[A\n",
      "Computing transition probabilities:  30%|██▉       | 8197/27770 [00:11<00:20, 941.61it/s]\u001b[A\n",
      "Computing transition probabilities:  30%|██▉       | 8322/27770 [00:11<00:19, 1014.63it/s]\u001b[A\n",
      "Computing transition probabilities:  30%|███       | 8426/27770 [00:11<00:19, 993.35it/s] \u001b[A\n",
      "Computing transition probabilities:  31%|███       | 8535/27770 [00:11<00:18, 1014.52it/s]\u001b[A\n",
      "Computing transition probabilities:  31%|███       | 8638/27770 [00:11<00:18, 1006.96it/s]\u001b[A\n",
      "Computing transition probabilities:  32%|███▏      | 8749/27770 [00:11<00:18, 1033.74it/s]\u001b[A\n",
      "Computing transition probabilities:  32%|███▏      | 8854/27770 [00:11<00:18, 1014.61it/s]\u001b[A\n",
      "Computing transition probabilities:  32%|███▏      | 8957/27770 [00:12<00:18, 1006.03it/s]\u001b[A\n",
      "Computing transition probabilities:  33%|███▎      | 9059/27770 [00:12<00:18, 993.86it/s] \u001b[A\n",
      "Computing transition probabilities:  33%|███▎      | 9191/27770 [00:12<00:17, 1070.37it/s]\u001b[A\n",
      "Computing transition probabilities:  34%|███▎      | 9303/27770 [00:12<00:17, 1083.43it/s]\u001b[A\n",
      "Computing transition probabilities:  34%|███▍      | 9413/27770 [00:12<00:17, 1065.06it/s]\u001b[A\n",
      "Computing transition probabilities:  34%|███▍      | 9521/27770 [00:12<00:17, 1027.77it/s]\u001b[A\n",
      "Computing transition probabilities:  35%|███▍      | 9637/27770 [00:12<00:17, 1061.42it/s]\u001b[A\n",
      "Computing transition probabilities:  35%|███▌      | 9745/27770 [00:12<00:17, 1036.16it/s]\u001b[A\n",
      "Computing transition probabilities:  36%|███▌      | 9876/27770 [00:12<00:16, 1099.38it/s]\u001b[A\n",
      "Computing transition probabilities:  36%|███▌      | 9988/27770 [00:13<00:16, 1098.53it/s]\u001b[A\n",
      "Computing transition probabilities:  36%|███▋      | 10103/27770 [00:13<00:15, 1112.67it/s]\u001b[A\n",
      "Computing transition probabilities:  37%|███▋      | 10216/27770 [00:13<00:16, 1082.27it/s]\u001b[A\n",
      "Computing transition probabilities:  37%|███▋      | 10338/27770 [00:13<00:15, 1115.38it/s]\u001b[A\n",
      "Computing transition probabilities:  38%|███▊      | 10455/27770 [00:13<00:15, 1123.88it/s]\u001b[A\n",
      "Computing transition probabilities:  38%|███▊      | 10568/27770 [00:13<00:15, 1116.01it/s]\u001b[A\n",
      "Computing transition probabilities:  38%|███▊      | 10686/27770 [00:13<00:15, 1132.97it/s]\u001b[A\n",
      "Computing transition probabilities:  39%|███▉      | 10805/27770 [00:13<00:14, 1148.79it/s]\u001b[A\n",
      "Computing transition probabilities:  39%|███▉      | 10921/27770 [00:13<00:15, 1113.16it/s]\u001b[A\n",
      "Computing transition probabilities:  40%|███▉      | 11033/27770 [00:13<00:15, 1081.06it/s]\u001b[A\n",
      "Computing transition probabilities:  40%|████      | 11142/27770 [00:14<00:15, 1073.04it/s]\u001b[A\n",
      "Computing transition probabilities:  41%|████      | 11289/27770 [00:14<00:14, 1166.97it/s]\u001b[A\n",
      "Computing transition probabilities:  41%|████      | 11409/27770 [00:14<00:13, 1170.17it/s]\u001b[A\n",
      "Computing transition probabilities:  42%|████▏     | 11544/27770 [00:14<00:13, 1216.98it/s]\u001b[A\n",
      "Computing transition probabilities:  42%|████▏     | 11668/27770 [00:14<00:13, 1181.93it/s]\u001b[A\n",
      "Computing transition probabilities:  43%|████▎     | 11808/27770 [00:14<00:12, 1237.95it/s]\u001b[A\n",
      "Computing transition probabilities:  43%|████▎     | 11934/27770 [00:14<00:12, 1235.09it/s]\u001b[A\n",
      "Computing transition probabilities:  43%|████▎     | 12059/27770 [00:14<00:13, 1201.70it/s]\u001b[A\n",
      "Computing transition probabilities:  44%|████▍     | 12183/27770 [00:14<00:12, 1209.98it/s]\u001b[A\n",
      "Computing transition probabilities:  44%|████▍     | 12312/27770 [00:15<00:12, 1228.65it/s]\u001b[A\n",
      "Computing transition probabilities:  45%|████▍     | 12439/27770 [00:15<00:12, 1228.47it/s]\u001b[A\n",
      "Computing transition probabilities:  45%|████▌     | 12574/27770 [00:15<00:12, 1260.45it/s]\u001b[A\n",
      "Computing transition probabilities:  46%|████▌     | 12706/27770 [00:15<00:11, 1272.72it/s]\u001b[A\n",
      "Computing transition probabilities:  46%|████▋     | 12848/27770 [00:15<00:11, 1303.98it/s]\u001b[A\n",
      "Computing transition probabilities:  47%|████▋     | 12979/27770 [00:15<00:11, 1290.15it/s]\u001b[A\n",
      "Computing transition probabilities:  47%|████▋     | 13117/27770 [00:15<00:11, 1313.89it/s]\u001b[A\n",
      "Computing transition probabilities:  48%|████▊     | 13249/27770 [00:15<00:11, 1268.98it/s]\u001b[A\n",
      "Computing transition probabilities:  48%|████▊     | 13378/27770 [00:15<00:11, 1274.73it/s]\u001b[A\n",
      "Computing transition probabilities:  49%|████▉     | 13538/27770 [00:15<00:10, 1356.17it/s]\u001b[A\n",
      "Computing transition probabilities:  49%|████▉     | 13680/27770 [00:16<00:10, 1368.68it/s]\u001b[A\n",
      "Computing transition probabilities:  50%|████▉     | 13823/27770 [00:16<00:10, 1384.24it/s]\u001b[A\n",
      "Computing transition probabilities:  50%|█████     | 13963/27770 [00:16<00:10, 1363.18it/s]\u001b[A\n",
      "Computing transition probabilities:  51%|█████     | 14122/27770 [00:16<00:09, 1423.85it/s]\u001b[A\n",
      "Computing transition probabilities:  51%|█████▏    | 14281/27770 [00:16<00:09, 1468.21it/s]\u001b[A\n",
      "Computing transition probabilities:  52%|█████▏    | 14430/27770 [00:16<00:09, 1447.21it/s]\u001b[A\n",
      "Computing transition probabilities:  53%|█████▎    | 14601/27770 [00:16<00:08, 1515.50it/s]\u001b[A\n",
      "Computing transition probabilities:  53%|█████▎    | 14756/27770 [00:16<00:08, 1525.02it/s]\u001b[A\n",
      "Computing transition probabilities:  54%|█████▎    | 14924/27770 [00:16<00:08, 1567.90it/s]\u001b[A\n",
      "Computing transition probabilities:  54%|█████▍    | 15111/27770 [00:16<00:07, 1645.51it/s]\u001b[A\n",
      "Computing transition probabilities:  55%|█████▌    | 15302/27770 [00:17<00:07, 1714.17it/s]\u001b[A\n",
      "Computing transition probabilities:  56%|█████▌    | 15476/27770 [00:17<00:07, 1718.91it/s]\u001b[A\n",
      "Computing transition probabilities:  56%|█████▋    | 15650/27770 [00:17<00:07, 1708.09it/s]\u001b[A\n",
      "Computing transition probabilities:  57%|█████▋    | 15822/27770 [00:17<00:07, 1687.47it/s]\u001b[A\n",
      "Computing transition probabilities:  58%|█████▊    | 15999/27770 [00:17<00:06, 1711.02it/s]\u001b[A\n",
      "Computing transition probabilities:  58%|█████▊    | 16182/27770 [00:17<00:06, 1736.25it/s]\u001b[A\n",
      "Computing transition probabilities:  59%|█████▉    | 16398/27770 [00:17<00:06, 1844.12it/s]\u001b[A\n",
      "Computing transition probabilities:  60%|█████▉    | 16585/27770 [00:17<00:06, 1804.74it/s]\u001b[A\n",
      "Computing transition probabilities:  61%|██████    | 16816/27770 [00:17<00:05, 1931.08it/s]\u001b[A\n",
      "Computing transition probabilities:  61%|██████▏   | 17013/27770 [00:17<00:05, 1913.10it/s]\u001b[A\n",
      "Computing transition probabilities:  62%|██████▏   | 17243/27770 [00:18<00:05, 2014.38it/s]\u001b[A\n",
      "Computing transition probabilities:  63%|██████▎   | 17472/27770 [00:18<00:04, 2084.19it/s]\u001b[A\n",
      "Computing transition probabilities:  64%|██████▎   | 17698/27770 [00:18<00:04, 2124.59it/s]\u001b[A\n",
      "Computing transition probabilities:  65%|██████▍   | 17913/27770 [00:18<00:04, 2093.78it/s]\u001b[A\n",
      "Computing transition probabilities:  65%|██████▌   | 18165/27770 [00:18<00:04, 2205.42it/s]\u001b[A\n",
      "Computing transition probabilities:  66%|██████▋   | 18411/27770 [00:18<00:04, 2267.20it/s]\u001b[A\n",
      "Computing transition probabilities:  67%|██████▋   | 18693/27770 [00:18<00:03, 2403.14it/s]\u001b[A\n",
      "Computing transition probabilities:  68%|██████▊   | 18947/27770 [00:18<00:03, 2440.99it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities:  69%|██████▉   | 19235/27770 [00:18<00:03, 2557.36it/s]\u001b[A\n",
      "Computing transition probabilities:  70%|███████   | 19525/27770 [00:18<00:03, 2648.70it/s]\u001b[A\n",
      "Computing transition probabilities:  71%|███████▏  | 19807/27770 [00:19<00:02, 2690.30it/s]\u001b[A\n",
      "Computing transition probabilities:  72%|███████▏  | 20125/27770 [00:19<00:02, 2808.07it/s]\u001b[A\n",
      "Computing transition probabilities:  74%|███████▍  | 20500/27770 [00:19<00:02, 3036.86it/s]\u001b[A\n",
      "Computing transition probabilities:  75%|███████▌  | 20851/27770 [00:19<00:02, 3162.82it/s]\u001b[A\n",
      "Computing transition probabilities:  76%|███████▋  | 21212/27770 [00:19<00:02, 3276.21it/s]\u001b[A\n",
      "Computing transition probabilities:  78%|███████▊  | 21654/27770 [00:19<00:01, 3551.85it/s]\u001b[A\n",
      "Computing transition probabilities:  80%|███████▉  | 22144/27770 [00:19<00:01, 3871.11it/s]\u001b[A\n",
      "Computing transition probabilities:  82%|████████▏ | 22647/27770 [00:19<00:01, 4157.75it/s]\u001b[A\n",
      "Computing transition probabilities:  84%|████████▍ | 23284/27770 [00:19<00:00, 4639.89it/s]\u001b[A\n",
      "Computing transition probabilities:  86%|████████▋ | 24009/27770 [00:20<00:00, 5201.09it/s]\u001b[A\n",
      "Computing transition probabilities:  90%|█████████ | 25022/27770 [00:20<00:00, 6089.75it/s]\u001b[A\n",
      "Computing transition probabilities: 100%|██████████| 27770/27770 [00:20<00:00, 1369.57it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "node2vec = Node2Vec(G_data, dimensions=128, walk_length=50, \n",
    "                    num_walks=50, workers=1, temp_folder='')\n",
    "# train node2vec model\n",
    "n2w_model = node2vec.fit(window=5, min_count=1)\n",
    "n2w_model.wv.save_word2vec_format('node_embv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "n2w_model = KeyedVectors.load_word2vec_format('node_emb', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_emb(data) :\n",
    "    node_emb = []\n",
    "    for i, j in zip(data['node_1'], data['node_2']) :\n",
    "        if i in G_data.nodes():\n",
    "            emb_i = n2w_model.wv[str(i)]\n",
    "        else :\n",
    "            emb_i = np.zeros(128)\n",
    "        if j in G_data.nodes():\n",
    "            emb_j = n2w_model.wv[str(j)]\n",
    "        else :\n",
    "            emb_j = np.zeros(128)\n",
    "        node_emb.append(np.hstack((emb_i, emb_j)))\n",
    "        #node_emb.append(emb_i * emb_j)\n",
    "    node_emb = np.asarray(node_emb)\n",
    "    return node_emb #np.vstack((node_emb, node_emb_))\n",
    "node_emb = get_node_emb(data_.drop(index = omissible_links_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585512, 256)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import GridSearchCV\n",
    "#import sklearn\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#xtrain, xtest, ytrain, ytest = train_test_split(node_emb, data['link'], \n",
    "#                                                test_size = 0.2, \n",
    "#                                                random_state = 35, stratify = data.link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(xtrain.shape, xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation sur les noeuds omis !!\n",
    "omitted_links = data_[data_.link == 1].iloc[omissible_links_index]\n",
    "omitted_emb = get_node_emb(omitted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 256)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = data_.drop(index = omissible_links_index).link.values\n",
    "omitted_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input, Concatenate\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 585512 samples, validate on 30000 samples\n",
      "Epoch 1/10\n",
      "585512/585512 [==============================] - 3s 5us/sample - loss: 0.2479 - f1_m: 0.9029 - val_loss: 0.1379 - val_f1_m: 0.9741\n",
      "Epoch 2/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.1238 - f1_m: 0.9598 - val_loss: 0.1387 - val_f1_m: 0.9757\n",
      "Epoch 3/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.1037 - f1_m: 0.9667 - val_loss: 0.1289 - val_f1_m: 0.9785\n",
      "Epoch 4/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0941 - f1_m: 0.9699 - val_loss: 0.1445 - val_f1_m: 0.9760\n",
      "Epoch 5/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0885 - f1_m: 0.9715 - val_loss: 0.1310 - val_f1_m: 0.9788\n",
      "Epoch 6/10\n",
      "585512/585512 [==============================] - 3s 4us/sample - loss: 0.0846 - f1_m: 0.9727 - val_loss: 0.1293 - val_f1_m: 0.9794\n",
      "Epoch 7/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0816 - f1_m: 0.9740 - val_loss: 0.1276 - val_f1_m: 0.9799\n",
      "Epoch 8/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0801 - f1_m: 0.9744 - val_loss: 0.1293 - val_f1_m: 0.9802\n",
      "Epoch 9/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0774 - f1_m: 0.9752 - val_loss: 0.1327 - val_f1_m: 0.9796\n",
      "Epoch 10/10\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0758 - f1_m: 0.9757 - val_loss: 0.1395 - val_f1_m: 0.9794\n",
      "0.9793835476627883\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "#model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1, input_shape=(2, 128), activation='relu'))\n",
    "#model.add(Bidirectional(LSTM(32, dropout=0.1,\n",
    "#                             recurrent_activation='relu'), \n",
    "#                        input_shape=(2, 128)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 1024\n",
    "n_epochs = 10\n",
    "history = model.fit(node_emb, y_train,\n",
    "                    batch_size=bs, epochs=n_epochs,\n",
    "                    validation_data=(omitted_emb, np.ones(len(omitted_emb))))\n",
    "y_pred = model.predict(omitted_emb)\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "print(sklearn.metrics.f1_score(np.ones(len(omitted_emb)), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test concaténation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 585512 samples, validate on 30000 samples\n",
      "Epoch 1/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.7212 - f1_m: 0.5808 - val_loss: 0.3603 - val_f1_m: 0.9042\n",
      "Epoch 2/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.5127 - f1_m: 0.7723 - val_loss: 0.3503 - val_f1_m: 0.9122\n",
      "Epoch 3/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.4082 - f1_m: 0.8372 - val_loss: 0.3311 - val_f1_m: 0.9262\n",
      "Epoch 4/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.3297 - f1_m: 0.8828 - val_loss: 0.2402 - val_f1_m: 0.9540\n",
      "Epoch 5/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.2656 - f1_m: 0.9101 - val_loss: 0.1859 - val_f1_m: 0.9675\n",
      "Epoch 6/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.2194 - f1_m: 0.9266 - val_loss: 0.1443 - val_f1_m: 0.9768\n",
      "Epoch 7/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1864 - f1_m: 0.9375 - val_loss: 0.1160 - val_f1_m: 0.9813\n",
      "Epoch 8/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1634 - f1_m: 0.9448 - val_loss: 0.0979 - val_f1_m: 0.9840\n",
      "Epoch 9/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1478 - f1_m: 0.9498 - val_loss: 0.0854 - val_f1_m: 0.9858\n",
      "Epoch 10/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1359 - f1_m: 0.9536 - val_loss: 0.0781 - val_f1_m: 0.9868\n",
      "Epoch 11/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1278 - f1_m: 0.9566 - val_loss: 0.0719 - val_f1_m: 0.9880\n",
      "Epoch 12/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1215 - f1_m: 0.9586 - val_loss: 0.0669 - val_f1_m: 0.9888\n",
      "Epoch 13/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.1164 - f1_m: 0.9605 - val_loss: 0.0648 - val_f1_m: 0.9891\n",
      "Epoch 14/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1112 - f1_m: 0.9624 - val_loss: 0.0605 - val_f1_m: 0.9900\n",
      "Epoch 15/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1075 - f1_m: 0.9638 - val_loss: 0.0585 - val_f1_m: 0.9902\n",
      "Epoch 16/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.1046 - f1_m: 0.9649 - val_loss: 0.0564 - val_f1_m: 0.9906\n",
      "Epoch 17/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.1015 - f1_m: 0.9659 - val_loss: 0.0546 - val_f1_m: 0.9911\n",
      "Epoch 18/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0983 - f1_m: 0.9670 - val_loss: 0.0526 - val_f1_m: 0.9914\n",
      "Epoch 19/50\n",
      "585512/585512 [==============================] - 2s 4us/sample - loss: 0.0964 - f1_m: 0.9679 - val_loss: 0.0525 - val_f1_m: 0.9913\n",
      "Epoch 20/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0940 - f1_m: 0.9685 - val_loss: 0.0505 - val_f1_m: 0.9917\n",
      "Epoch 21/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0920 - f1_m: 0.9692 - val_loss: 0.0484 - val_f1_m: 0.9922\n",
      "Epoch 22/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0891 - f1_m: 0.9702 - val_loss: 0.0478 - val_f1_m: 0.9924\n",
      "Epoch 23/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0876 - f1_m: 0.9711 - val_loss: 0.0472 - val_f1_m: 0.9923\n",
      "Epoch 24/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0863 - f1_m: 0.9713 - val_loss: 0.0466 - val_f1_m: 0.9925\n",
      "Epoch 25/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0845 - f1_m: 0.9722 - val_loss: 0.0453 - val_f1_m: 0.9928\n",
      "Epoch 26/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0831 - f1_m: 0.9726 - val_loss: 0.0447 - val_f1_m: 0.9928\n",
      "Epoch 27/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0817 - f1_m: 0.9732 - val_loss: 0.0450 - val_f1_m: 0.9927\n",
      "Epoch 28/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0803 - f1_m: 0.9737 - val_loss: 0.0416 - val_f1_m: 0.9936\n",
      "Epoch 29/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0789 - f1_m: 0.9742 - val_loss: 0.0444 - val_f1_m: 0.9930\n",
      "Epoch 30/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0778 - f1_m: 0.9747 - val_loss: 0.0420 - val_f1_m: 0.9934\n",
      "Epoch 31/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0766 - f1_m: 0.9750 - val_loss: 0.0422 - val_f1_m: 0.9933\n",
      "Epoch 32/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0757 - f1_m: 0.9754 - val_loss: 0.0418 - val_f1_m: 0.9933\n",
      "Epoch 33/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0743 - f1_m: 0.9759 - val_loss: 0.0415 - val_f1_m: 0.9934\n",
      "Epoch 34/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0732 - f1_m: 0.9763 - val_loss: 0.0404 - val_f1_m: 0.9937\n",
      "Epoch 35/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0726 - f1_m: 0.9765 - val_loss: 0.0414 - val_f1_m: 0.9934\n",
      "Epoch 36/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0714 - f1_m: 0.9769 - val_loss: 0.0405 - val_f1_m: 0.9935\n",
      "Epoch 37/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0707 - f1_m: 0.9771 - val_loss: 0.0409 - val_f1_m: 0.9935\n",
      "Epoch 38/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0705 - f1_m: 0.9771 - val_loss: 0.0398 - val_f1_m: 0.9937\n",
      "Epoch 39/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0691 - f1_m: 0.9777 - val_loss: 0.0404 - val_f1_m: 0.9937\n",
      "Epoch 40/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0683 - f1_m: 0.9781 - val_loss: 0.0398 - val_f1_m: 0.9937\n",
      "Epoch 41/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0676 - f1_m: 0.9782 - val_loss: 0.0398 - val_f1_m: 0.9939\n",
      "Epoch 42/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0672 - f1_m: 0.9786 - val_loss: 0.0392 - val_f1_m: 0.9939\n",
      "Epoch 43/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0661 - f1_m: 0.9789 - val_loss: 0.0396 - val_f1_m: 0.9939\n",
      "Epoch 44/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0654 - f1_m: 0.9791 - val_loss: 0.0385 - val_f1_m: 0.9940\n",
      "Epoch 45/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0644 - f1_m: 0.9794 - val_loss: 0.0395 - val_f1_m: 0.9939\n",
      "Epoch 46/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0644 - f1_m: 0.9794 - val_loss: 0.0383 - val_f1_m: 0.9940\n",
      "Epoch 47/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0635 - f1_m: 0.9796 - val_loss: 0.0394 - val_f1_m: 0.9938\n",
      "Epoch 48/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0626 - f1_m: 0.9801 - val_loss: 0.0389 - val_f1_m: 0.9940\n",
      "Epoch 49/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0620 - f1_m: 0.9804 - val_loss: 0.0400 - val_f1_m: 0.9937\n",
      "Epoch 50/50\n",
      "585512/585512 [==============================] - 2s 3us/sample - loss: 0.0614 - f1_m: 0.9804 - val_loss: 0.0378 - val_f1_m: 0.9941\n"
     ]
    }
   ],
   "source": [
    "#Input image\n",
    "input_emb = Input(shape=(256,))\n",
    "input_attr = Input(shape=(15,))\n",
    "\n",
    "encoded_1 = Dense(units=64, activation='relu')(input_emb)\n",
    "encoded_1 = Dropout(0.1)(encoded_1)\n",
    "encoded_2 = Dense(units=30, activation='relu')(input_attr)\n",
    "encoded_2 = Dropout(0.1)(encoded_2)\n",
    "encoded = Concatenate()([encoded_1, encoded_2])\n",
    "\n",
    "output = Dense(units=16, activation='relu')(encoded)\n",
    "output = Dropout(0.1)(output)\n",
    "output = Dense(units=1, activation='sigmoid', name='autoencoder')(output)\n",
    "\n",
    "autoencoder = Model(inputs = [input_emb, input_attr], outputs = output)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics= [f1_m])\n",
    "\n",
    "history = autoencoder.fit([node_emb, train], y_train,\n",
    "                epochs=50,\n",
    "                batch_size=50000,\n",
    "                validation_data=([omitted_emb, val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585512, 16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pub_year_2</th>\n",
       "      <th>authors_x</th>\n",
       "      <th>node_2</th>\n",
       "      <th>pub_year_1</th>\n",
       "      <th>authors_y</th>\n",
       "      <th>node_1</th>\n",
       "      <th>link</th>\n",
       "      <th>diff_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Paul S. Aspinwall]</td>\n",
       "      <td>1001</td>\n",
       "      <td>1993</td>\n",
       "      <td>[C.N. Pope]</td>\n",
       "      <td>9309125</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Paul S. Aspinwall]</td>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>[A. de Rujula]</td>\n",
       "      <td>9405191</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Paul S. Aspinwall]</td>\n",
       "      <td>1001</td>\n",
       "      <td>1997</td>\n",
       "      <td>[Yoonbai Kim]</td>\n",
       "      <td>9708038</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2000</td>\n",
       "      <td>[M. Cvetic,  H. Lu,  C.N. Pope]</td>\n",
       "      <td>1002</td>\n",
       "      <td>1997</td>\n",
       "      <td>[]</td>\n",
       "      <td>9701157</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Y.S. Myung,  Gungwon Kang]</td>\n",
       "      <td>1003</td>\n",
       "      <td>1992</td>\n",
       "      <td>[H.Itoyama,  B.Razzaghe-Ashrafi]</td>\n",
       "      <td>9204007</td>\n",
       "      <td>0</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55509</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...</td>\n",
       "      <td>11011</td>\n",
       "      <td>1994</td>\n",
       "      <td>[R.P. Manvelyan,  R.L. Mkrtchyan (Yerevan Phys...</td>\n",
       "      <td>9401032</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55510</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...</td>\n",
       "      <td>11011</td>\n",
       "      <td>1995</td>\n",
       "      <td>[Ralf Kerschner (T\"ubingen)]</td>\n",
       "      <td>9506055</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55511</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...</td>\n",
       "      <td>11011</td>\n",
       "      <td>1996</td>\n",
       "      <td>[Sergio Ferrara,  Ramzi R. Khuri,  Ruben Minas...</td>\n",
       "      <td>9602102</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55512</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...</td>\n",
       "      <td>11011</td>\n",
       "      <td>1997</td>\n",
       "      <td>[Sergio Ferrara,  Murat Gunaydin]</td>\n",
       "      <td>9708025</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55514</td>\n",
       "      <td>2000</td>\n",
       "      <td>[Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...</td>\n",
       "      <td>11011</td>\n",
       "      <td>1999</td>\n",
       "      <td>[A. Giveon,  D. Kutasov]</td>\n",
       "      <td>9909110</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8656 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pub_year_2                                          authors_x  node_2  \\\n",
       "17          2000                                [Paul S. Aspinwall]    1001   \n",
       "18          2000                                [Paul S. Aspinwall]    1001   \n",
       "22          2000                                [Paul S. Aspinwall]    1001   \n",
       "69          2000                    [M. Cvetic,  H. Lu,  C.N. Pope]    1002   \n",
       "90          2000                        [Y.S. Myung,  Gungwon Kang]    1003   \n",
       "...          ...                                                ...     ...   \n",
       "55509       2000  [Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...   11011   \n",
       "55510       2000  [Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...   11011   \n",
       "55511       2000  [Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...   11011   \n",
       "55512       2000  [Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...   11011   \n",
       "55514       2000  [Rabin Banerjee,  Biswajit Chakraborty,  Tomy ...   11011   \n",
       "\n",
       "      pub_year_1                                          authors_y   node_1  \\\n",
       "17          1993                                        [C.N. Pope]  9309125   \n",
       "18          1994                                     [A. de Rujula]  9405191   \n",
       "22          1997                                      [Yoonbai Kim]  9708038   \n",
       "69          1997                                                 []  9701157   \n",
       "90          1992                   [H.Itoyama,  B.Razzaghe-Ashrafi]  9204007   \n",
       "...          ...                                                ...      ...   \n",
       "55509       1994  [R.P. Manvelyan,  R.L. Mkrtchyan (Yerevan Phys...  9401032   \n",
       "55510       1995                       [Ralf Kerschner (T\"ubingen)]  9506055   \n",
       "55511       1996  [Sergio Ferrara,  Ramzi R. Khuri,  Ruben Minas...  9602102   \n",
       "55512       1997                  [Sergio Ferrara,  Murat Gunaydin]  9708025   \n",
       "55514       1999                           [A. Giveon,  D. Kutasov]  9909110   \n",
       "\n",
       "       link diff_year  \n",
       "17        0        -7  \n",
       "18        0        -6  \n",
       "22        0        -3  \n",
       "69        0        -3  \n",
       "90        0        -8  \n",
       "...     ...       ...  \n",
       "55509     0        -6  \n",
       "55510     0        -5  \n",
       "55511     0        -4  \n",
       "55512     0        -3  \n",
       "55514     0        -1  \n",
       "\n",
       "[8656 rows x 8 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omit = tmp.iloc[omissible_links_index]\n",
    "omit [omit.diff_year < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9934237015165749\n"
     ]
    }
   ],
   "source": [
    "y_pred = autoencoder.predict([omitted_emb, val])[:, 0]\n",
    "y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "print(sklearn.metrics.f1_score(np.ones(len(omitted_emb)), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction sur les données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.DataFrame({'node_1':[int(elt[0]) for elt in testing_set], \n",
    "                     'node_2':[int(elt[1]) for elt in testing_set]})\n",
    "emb_test = get_node_emb(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_test = [tuple(elt[0 : 2]) for elt in testing_set]\n",
    "#y_train = [int(elt[-1]) for elt in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32648/32648 [00:04<00:00, 7483.54it/s] \n"
     ]
    }
   ],
   "source": [
    "with Pool(4) as p: \n",
    "    features_test = p.map(get_features, pairs_test)\n",
    "features_test = np.array(features_test)\n",
    "test_tr = []\n",
    "for elt in tqdm.tqdm(pairs_test) :\n",
    "    test_tr.append(get_graph_feature(elt))\n",
    "test_tr = np.asarray(test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(585512, 15) (32648, 15)\n"
     ]
    }
   ],
   "source": [
    "## prédiction sur le test\n",
    "test = np.hstack((test_tr, features_test))\n",
    "test = scaler.transform(test)\n",
    "print(train.shape, test.shape)\n",
    "y_test = autoencoder.predict([emb_test, test])[:, 0]\n",
    "y_test = np.where(y_test > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-entraînement sur tout le jeu de données\n",
    "train = np.hstack((train_tr, features_nodes))\n",
    "val = np.hstack((val_tr, features_val))\n",
    "train = np.vstack((train, val))\n",
    "train = scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "615512/615512 [==============================] - 2s 3us/sample - loss: 0.6906 - f1_m: 0.6911\n",
      "Epoch 2/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6893 - f1_m: 0.7046\n",
      "Epoch 3/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7049\n",
      "Epoch 4/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7049\n",
      "Epoch 5/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 6/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7049\n",
      "Epoch 7/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7049\n",
      "Epoch 8/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7049\n",
      "Epoch 9/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 10/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 11/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 12/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 13/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 14/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 15/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 16/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 17/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 18/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 19/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 20/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 21/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 22/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 23/30\n",
      "615512/615512 [==============================] - 1s 2us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 24/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 25/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 26/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 27/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 28/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 29/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n",
      "Epoch 30/30\n",
      "615512/615512 [==============================] - 1s 1us/sample - loss: 0.6892 - f1_m: 0.7050\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import tensorflow.keras.backend as K\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=[f1_m])\n",
    "bs = 2048\n",
    "n_epochs = 30\n",
    "history = model.fit(train, np.array(y_train),\n",
    "                    batch_size=bs, epochs=n_epochs)\n",
    "y_pred = model.predict(test)\n",
    "#y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "#print(sklearn.metrics.f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_SVM = zip(range(len(testing_set)), y_test)\n",
    "with open(\"improved_predictions.csv\",\"w\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('id', 'category'))\n",
    "    for row in predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
